{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\mkaze\\AppData\\Local\\Temp\\ipykernel_9172\\2956580225.py\", line 4, in <module>\n",
      "    from pyspark.ml.functions import vector_to_array\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\functions.py\", line 21, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\compat\\__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\compat\\pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\mkaze\\AppData\\Local\\Temp\\ipykernel_9172\\2956580225.py\", line 4, in <module>\n",
      "    from pyspark.ml.functions import vector_to_array\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\functions.py\", line 21, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row, Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from collections import defaultdict\n",
    "from operator import add\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jre1.8.0_441\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\Spark\\\\spark-3.5.5-bin-hadoop3\" \n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"outlier_dataset_15000_1500.csv\"\n",
    "nodes_list = [1, 2, 4, 8, 16]\n",
    "p = 2\n",
    "m_max = 5\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o12214.javaToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 18.0 failed 4 times, most recent failure: Lost task 1.3 in stage 18.0 (TID 69) (192.168.1.104 executor 0): java.io.IOException: Failed to delete original file 'C:\\Users\\mkaze\\AppData\\Local\\Temp\\spark-d5fdcd7b-a4ba-4b9c-9748-0572b9585c4f\\executor-c9d1856f-f06f-4320-a47b-c84954501857\\spark-6282bbb2-a427-4017-83a2-127a03f3d1f5\\broadcast2727097521926426241' after copy to 'C:\\Users\\mkaze\\AppData\\Local\\Temp\\spark-d5fdcd7b-a4ba-4b9c-9748-0572b9585c4f\\executor-c9d1856f-f06f-4320-a47b-c84954501857\\blockmgr-b88d7803-89c3-4617-8de4-60fb1bbdcf0a\\03\\broadcast_22_python'\r\n\tat org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2477)\r\n\tat org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2448)\r\n\tat org.apache.spark.storage.DiskStore.moveFileToBlock(DiskStore.scala:152)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.saveToDiskStore(BlockManager.scala:487)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.$anonfun$save$1(BlockManager.scala:407)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.save(BlockManager.scala:490)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$2(PythonRDD.scala:798)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$1(PythonRDD.scala:800)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\r\n\tat org.apache.spark.api.python.PythonBroadcast.readObject(PythonRDD.scala:787)\r\n\tat sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat java.io.ObjectStreamClass.invokeReadObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$unBlockifyObject$4(TorrentBroadcast.scala:382)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:384)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:285)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\r\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\r\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$22(PythonRunner.scala:442)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.IOException: Failed to delete original file 'C:\\Users\\mkaze\\AppData\\Local\\Temp\\spark-d5fdcd7b-a4ba-4b9c-9748-0572b9585c4f\\executor-c9d1856f-f06f-4320-a47b-c84954501857\\spark-6282bbb2-a427-4017-83a2-127a03f3d1f5\\broadcast2727097521926426241' after copy to 'C:\\Users\\mkaze\\AppData\\Local\\Temp\\spark-d5fdcd7b-a4ba-4b9c-9748-0572b9585c4f\\executor-c9d1856f-f06f-4320-a47b-c84954501857\\blockmgr-b88d7803-89c3-4617-8de4-60fb1bbdcf0a\\03\\broadcast_22_python'\r\n\tat org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2477)\r\n\tat org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2448)\r\n\tat org.apache.spark.storage.DiskStore.moveFileToBlock(DiskStore.scala:152)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.saveToDiskStore(BlockManager.scala:487)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.$anonfun$save$1(BlockManager.scala:407)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.save(BlockManager.scala:490)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$2(PythonRDD.scala:798)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$1(PythonRDD.scala:800)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\r\n\tat org.apache.spark.api.python.PythonBroadcast.readObject(PythonRDD.scala:787)\r\n\tat sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat java.io.ObjectStreamClass.invokeReadObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$unBlockifyObject$4(TorrentBroadcast.scala:382)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:384)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:285)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\r\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\r\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$22(PythonRunner.scala:442)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m df_binned \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(binned_rdd, schema)\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Count per cube (use tuple as key)\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m cube_count \u001b[38;5;241m=\u001b[39m df_binned\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m r: (\u001b[38;5;28mtuple\u001b[39m(r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcube\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mreduceByKey(add)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Build density grid DataFrame\u001b[39;00m\n\u001b[0;32m     47\u001b[0m dg_schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m     48\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcube\u001b[39m\u001b[38;5;124m\"\u001b[39m, ArrayType(IntegerType(), containsNull\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m     49\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdensity\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     50\u001b[0m ])\n",
      "File \u001b[1;32mc:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:214\u001b[0m, in \u001b[0;36mDataFrame.rdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m<class 'pyspark.rdd.RDD'>\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m     jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mjavaToPython()\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;241m=\u001b[39m RDD(\n\u001b[0;32m    216\u001b[0m         jrdd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_sc, BatchedSerializer(CPickleSerializer())\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd\n",
      "File \u001b[1;32mc:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o12214.javaToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 18.0 failed 4 times, most recent failure: Lost task 1.3 in stage 18.0 (TID 69) (192.168.1.104 executor 0): java.io.IOException: Failed to delete original file 'C:\\Users\\mkaze\\AppData\\Local\\Temp\\spark-d5fdcd7b-a4ba-4b9c-9748-0572b9585c4f\\executor-c9d1856f-f06f-4320-a47b-c84954501857\\spark-6282bbb2-a427-4017-83a2-127a03f3d1f5\\broadcast2727097521926426241' after copy to 'C:\\Users\\mkaze\\AppData\\Local\\Temp\\spark-d5fdcd7b-a4ba-4b9c-9748-0572b9585c4f\\executor-c9d1856f-f06f-4320-a47b-c84954501857\\blockmgr-b88d7803-89c3-4617-8de4-60fb1bbdcf0a\\03\\broadcast_22_python'\r\n\tat org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2477)\r\n\tat org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2448)\r\n\tat org.apache.spark.storage.DiskStore.moveFileToBlock(DiskStore.scala:152)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.saveToDiskStore(BlockManager.scala:487)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.$anonfun$save$1(BlockManager.scala:407)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.save(BlockManager.scala:490)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$2(PythonRDD.scala:798)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$1(PythonRDD.scala:800)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\r\n\tat org.apache.spark.api.python.PythonBroadcast.readObject(PythonRDD.scala:787)\r\n\tat sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat java.io.ObjectStreamClass.invokeReadObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$unBlockifyObject$4(TorrentBroadcast.scala:382)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:384)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:285)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\r\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\r\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$22(PythonRunner.scala:442)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.IOException: Failed to delete original file 'C:\\Users\\mkaze\\AppData\\Local\\Temp\\spark-d5fdcd7b-a4ba-4b9c-9748-0572b9585c4f\\executor-c9d1856f-f06f-4320-a47b-c84954501857\\spark-6282bbb2-a427-4017-83a2-127a03f3d1f5\\broadcast2727097521926426241' after copy to 'C:\\Users\\mkaze\\AppData\\Local\\Temp\\spark-d5fdcd7b-a4ba-4b9c-9748-0572b9585c4f\\executor-c9d1856f-f06f-4320-a47b-c84954501857\\blockmgr-b88d7803-89c3-4617-8de4-60fb1bbdcf0a\\03\\broadcast_22_python'\r\n\tat org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2477)\r\n\tat org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2448)\r\n\tat org.apache.spark.storage.DiskStore.moveFileToBlock(DiskStore.scala:152)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.saveToDiskStore(BlockManager.scala:487)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.$anonfun$save$1(BlockManager.scala:407)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\r\n\tat org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.save(BlockManager.scala:490)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$2(PythonRDD.scala:798)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$readObject$1(PythonRDD.scala:800)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\r\n\tat org.apache.spark.api.python.PythonBroadcast.readObject(PythonRDD.scala:787)\r\n\tat sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat java.io.ObjectStreamClass.invokeReadObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject(Unknown Source)\r\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$unBlockifyObject$4(TorrentBroadcast.scala:382)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:384)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:285)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\r\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\r\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\r\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$22(PythonRunner.scala:442)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "# Loop over different simulated node counts\n",
    "for cores in nodes_list:\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Start Spark session with given number of local cores\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(f\"LOF_Eval_{cores}cores\") \\\n",
    "        .master(f\"local-cluster[{cores},2,2048]\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # 1) Load data and cache\n",
    "    df = spark.read.csv(data_path, header=True, inferSchema=True).cache()\n",
    "    cols = df.columns\n",
    "    label_col = cols[-1]\n",
    "    feature_cols = cols[:-1]\n",
    "\n",
    "    # 2) Normalize features to [0,1]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "    df_vec = assembler.transform(df).cache()\n",
    "    scaler = MinMaxScaler(inputCol=\"features_vec\", outputCol=\"scaled_vec\")\n",
    "    scaler_model = scaler.fit(df_vec)\n",
    "    df_scaled = scaler_model.transform(df_vec).cache()\n",
    "    df_scaled = df_scaled.withColumn(\"scaled_arr\", vector_to_array(\"scaled_vec\"))\n",
    "    scaled_exprs = [F.col(\"scaled_arr\")[i].alias(feature_cols[i]) for i in range(len(feature_cols))]\n",
    "    df_norm = df_scaled.select(*scaled_exprs, F.col(label_col)).cache()\n",
    "\n",
    "    # 3) Grid-based density via binning\n",
    "    width = 1.0 / p\n",
    "    def to_binned_row(row):\n",
    "        bins = [min(max(int(math.ceil(row[f] / width)), 1), p) for f in feature_cols]\n",
    "        rec = {f\"{feature_cols[i]}_bin\": bins[i] for i in range(len(feature_cols))}\n",
    "        rec[\"cube\"] = bins  # list of ints\n",
    "        return Row(**rec)\n",
    "    binned_rdd = df_norm.select(*feature_cols).rdd.map(to_binned_row)\n",
    "\n",
    "    # Explicit schema for binned DataFrame\n",
    "    bin_fields = [StructField(f\"{feature_cols[i]}_bin\", IntegerType(), False) for i in range(len(feature_cols))]\n",
    "    schema = StructType(bin_fields + [StructField(\"cube\", ArrayType(IntegerType(), containsNull=False), False)])\n",
    "    df_binned = spark.createDataFrame(binned_rdd, schema).cache()\n",
    "\n",
    "    # Count per cube (use tuple as key)\n",
    "    cube_count = df_binned.rdd.map(lambda r: (tuple(r[\"cube\"]), 1)).reduceByKey(add)\n",
    "    # Build density grid DataFrame\n",
    "    dg_schema = StructType([\n",
    "        StructField(\"cube\", ArrayType(IntegerType(), containsNull=False), False),\n",
    "        StructField(\"density\", IntegerType(), False)\n",
    "    ])\n",
    "    dg = (cube_count\n",
    "          .map(lambda x: (list(x[0]), x[1]))\n",
    "          .toDF(schema=dg_schema)\n",
    "          .cache())\n",
    "\n",
    "    # 4) mRMRD feature selection\n",
    "    bin_cols = [f\"{c}_bin\" for c in feature_cols]\n",
    "    df_dens = df_binned.join(dg, on=\"cube\").select(*bin_cols, \"density\").cache()\n",
    "    N = df_dens.count()\n",
    "\n",
    "    # Compute relevance MI(feature; density)\n",
    "    def rel_map(row):\n",
    "        y = row[\"density\"]\n",
    "        for f in feature_cols:\n",
    "            yield ((f, row[f\"{f}_bin\"], y), 1)\n",
    "    cont_rel = df_dens.rdd.flatMap(rel_map).reduceByKey(add).collect()\n",
    "    n_xy, n_x, n_y = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "    for (f, b, y), cnt in cont_rel:\n",
    "        n_xy[(f, b, y)] = cnt\n",
    "        n_x[(f, b)] += cnt\n",
    "        n_y[y] += cnt\n",
    "    mi_fd = {f: sum((cnt/N) * math.log((cnt/N)/((n_x[(f,b)]/N)*(n_y[y]/N)))\n",
    "                 for ((ff,b,y),cnt) in n_xy.items() if ff==f)\n",
    "             for f in feature_cols}\n",
    "    Th = sum(mi_fd.values()) / len(mi_fd)\n",
    "\n",
    "    selected, candidates, mi_red = [], feature_cols.copy(), {}\n",
    "    while candidates and len(selected) < m_max:\n",
    "        if not selected:\n",
    "            best = max(candidates, key=lambda x: mi_fd[x])\n",
    "        else:\n",
    "            s = selected[-1]\n",
    "            def red_map(row):\n",
    "                bs = row[f\"{s}_bin\"]\n",
    "                for j in candidates:\n",
    "                    yield ((j, row[f\"{j}_bin\"], bs), 1)\n",
    "            cont_rs = df_binned.rdd.flatMap(red_map).reduceByKey(add).collect()\n",
    "            n_xy_rs, n_x_rs, n_s_rs = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "            for (j, b_j, b_s), cnt in cont_rs:\n",
    "                n_xy_rs[(j,b_j,b_s)] = cnt\n",
    "                n_x_rs[(j,b_j)] += cnt\n",
    "                n_s_rs[b_s] += cnt\n",
    "            for j in candidates:\n",
    "                if (j,s) not in mi_red:\n",
    "                    I_js = sum((cnt/N) * math.log((cnt/N)/((n_x_rs[(j,b_j)]/N)*(n_s_rs[b_s]/N)))\n",
    "                               for ((jj,b_j,b_s),cnt) in n_xy_rs.items() if jj==j)\n",
    "                    mi_red[(j,s)] = mi_red[(s,j)] = I_js\n",
    "            best, best_score = None, float('-inf')\n",
    "            for j in candidates:\n",
    "                rel = mi_fd[j]\n",
    "                red_avg = sum(mi_red[(j,t)] for t in selected)/len(selected) if selected else 0.0\n",
    "                score = rel - red_avg\n",
    "                if score > best_score:\n",
    "                    best, best_score = j, score\n",
    "        if mi_fd[best] < Th:\n",
    "            break\n",
    "        selected.append(best)\n",
    "        candidates.remove(best)\n",
    "    print(f\"Final mRMRD subspace: {selected}\")\n",
    "\n",
    "    # 5) LOF computation\n",
    "    df_proj = df_norm.select(*selected, label_col).withColumn(\"id\", F.monotonically_increasing_id()).cache()\n",
    "    assembler2 = VectorAssembler(inputCols=selected, outputCol=\"features_vec\")\n",
    "    df_vec2 = assembler2.transform(df_proj).select(\"id\",\"features_vec\").cache()\n",
    "    lsh = BucketedRandomProjectionLSH(inputCol=\"features_vec\", outputCol=\"hashes\", bucketLength=math.sqrt(len(selected))/2)\n",
    "    model = lsh.fit(df_vec2)\n",
    "    max_dist = math.sqrt(len(selected))\n",
    "    pairs = (model.approxSimilarityJoin(df_vec2, df_vec2, max_dist, distCol=\"dist\")\n",
    "             .select(F.col(\"datasetA.id\").alias(\"pid\"), F.col(\"datasetB.id\").alias(\"oid\"), \"dist\")\n",
    "             .filter(\"pid < oid\"))\n",
    "    pairs = pairs.unionByName(pairs.selectExpr(\"oid as pid\",\"pid as oid\",\"dist\"))\n",
    "    w = F.row_number().over(Window.partitionBy(\"pid\").orderBy(\"dist\"))\n",
    "    knn = pairs.withColumn(\"rn\",w).filter(F.col(\"rn\")<=k).select(\"pid\",\"oid\",\"dist\").cache()\n",
    "    kdist = knn.groupBy(\"oid\").agg(F.max(\"dist\").alias(\"kdist\")).cache()\n",
    "    rd = knn.join(kdist, on=\"oid\").withColumn(\"reach_dist\",F.greatest(\"dist\",\"kdist\")).cache()\n",
    "    lrd = rd.groupBy(\"pid\").agg((F.lit(k)/F.sum(\"reach_dist\")).alias(\"lrd\")).cache()\n",
    "    lof = (rd.join(lrd.select(F.col(\"pid\").alias(\"oid\"),\"lrd\"),on=\"oid\")\n",
    "           .groupBy(\"pid\").agg(F.avg(\"lrd\").alias(\"avg_lrd_o\"))\n",
    "           .join(lrd,on=\"pid\")\n",
    "           .withColumn(\"LOF\",F.col(\"avg_lrd_o\")/F.col(\"lrd\")).cache())\n",
    "\n",
    "    # 6) AUC evaluation\n",
    "    score_label = (df_proj.join(lof.withColumnRenamed(\"pid\",\"id\").select(\"id\",\"LOF\"),on=\"id\")\n",
    "                   .select(\"LOF\", label_col)\n",
    "                   .rdd.map(lambda r: (float(r[0]), float(r[1]))))\n",
    "    metrics = BinaryClassificationMetrics(score_label)\n",
    "    auc = metrics.areaUnderROC\n",
    "\n",
    "    # Print results\n",
    "    t1 = time.time()\n",
    "    print(f\"Cores={cores:2d}  Time={(t1-t0):.1f}s  AUC={auc:.4f}\")\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
