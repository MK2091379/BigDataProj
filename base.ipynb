{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row, Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from collections import defaultdict\n",
    "from operator import add\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jre1.8.0_441\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\Spark\\\\spark-3.5.5-bin-hadoop3\" \n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"outlier_dataset_200_200.csv\"\n",
    "nodes_list = [1, 2, 4, 8, 16, 32, 64]\n",
    "p = 2\n",
    "m_max = 5\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over different simulated node counts\n",
    "for cores in nodes_list:\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Start Spark session with given number of local cores\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(f\"LOF_Eval_{cores}cores\") \\\n",
    "        .master(f\"local[{cores}]\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # 1) Load data and cache\n",
    "    df = spark.read.csv(data_path, header=True, inferSchema=True).cache()\n",
    "    cols = df.columns\n",
    "    label_col = cols[-1]\n",
    "    feature_cols = cols[:-1]\n",
    "\n",
    "    # 2) Normalize features to [0,1]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "    df_vec = assembler.transform(df).cache()\n",
    "    scaler = MinMaxScaler(inputCol=\"features_vec\", outputCol=\"scaled_vec\")\n",
    "    scaler_model = scaler.fit(df_vec)\n",
    "    df_scaled = scaler_model.transform(df_vec).cache()\n",
    "    df_scaled = df_scaled.withColumn(\"scaled_arr\", vector_to_array(\"scaled_vec\"))\n",
    "    scaled_exprs = [F.col(\"scaled_arr\")[i].alias(feature_cols[i]) for i in range(len(feature_cols))]\n",
    "    df_norm = df_scaled.select(*scaled_exprs, F.col(label_col)).cache()\n",
    "\n",
    "    # 3) Grid-based density via binning\n",
    "    width = 1.0 / p\n",
    "    def to_binned_row(row):\n",
    "        bins = [min(max(int(math.ceil(row[f] / width)), 1), p) for f in feature_cols]\n",
    "        rec = {f\"{feature_cols[i]}_bin\": bins[i] for i in range(len(feature_cols))}\n",
    "        rec[\"cube\"] = bins  # list of ints\n",
    "        return Row(**rec)\n",
    "    binned_rdd = df_norm.select(*feature_cols).rdd.map(to_binned_row)\n",
    "\n",
    "    # Explicit schema for binned DataFrame\n",
    "    bin_fields = [StructField(f\"{feature_cols[i]}_bin\", IntegerType(), False) for i in range(len(feature_cols))]\n",
    "    schema = StructType(bin_fields + [StructField(\"cube\", ArrayType(IntegerType(), containsNull=False), False)])\n",
    "    df_binned = spark.createDataFrame(binned_rdd, schema).cache()\n",
    "\n",
    "    # Count per cube (use tuple as key)\n",
    "    cube_count = df_binned.rdd.map(lambda r: (tuple(r[\"cube\"]), 1)).reduceByKey(add)\n",
    "    # Build density grid DataFrame\n",
    "    dg_schema = StructType([\n",
    "        StructField(\"cube\", ArrayType(IntegerType(), containsNull=False), False),\n",
    "        StructField(\"density\", IntegerType(), False)\n",
    "    ])\n",
    "    dg = (cube_count\n",
    "          .map(lambda x: (list(x[0]), x[1]))\n",
    "          .toDF(schema=dg_schema)\n",
    "          .cache())\n",
    "\n",
    "    # 4) mRMRD feature selection\n",
    "    bin_cols = [f\"{c}_bin\" for c in feature_cols]\n",
    "    df_dens = df_binned.join(dg, on=\"cube\").select(*bin_cols, \"density\").cache()\n",
    "    N = df_dens.count()\n",
    "\n",
    "    # Compute relevance MI(feature; density)\n",
    "    def rel_map(row):\n",
    "        y = row[\"density\"]\n",
    "        for f in feature_cols:\n",
    "            yield ((f, row[f\"{f}_bin\"], y), 1)\n",
    "    cont_rel = df_dens.rdd.flatMap(rel_map).reduceByKey(add).collect()\n",
    "    n_xy, n_x, n_y = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "    for (f, b, y), cnt in cont_rel:\n",
    "        n_xy[(f, b, y)] = cnt\n",
    "        n_x[(f, b)] += cnt\n",
    "        n_y[y] += cnt\n",
    "    mi_fd = {f: sum((cnt/N) * math.log((cnt/N)/((n_x[(f,b)]/N)*(n_y[y]/N)))\n",
    "                 for ((ff,b,y),cnt) in n_xy.items() if ff==f)\n",
    "             for f in feature_cols}\n",
    "    Th = sum(mi_fd.values()) / len(mi_fd)\n",
    "\n",
    "    selected, candidates, mi_red = [], feature_cols.copy(), {}\n",
    "    while candidates and len(selected) < m_max:\n",
    "        if not selected:\n",
    "            best = max(candidates, key=lambda x: mi_fd[x])\n",
    "        else:\n",
    "            s = selected[-1]\n",
    "            def red_map(row):\n",
    "                bs = row[f\"{s}_bin\"]\n",
    "                for j in candidates:\n",
    "                    yield ((j, row[f\"{j}_bin\"], bs), 1)\n",
    "            cont_rs = df_binned.rdd.flatMap(red_map).reduceByKey(add).collect()\n",
    "            n_xy_rs, n_x_rs, n_s_rs = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "            for (j, b_j, b_s), cnt in cont_rs:\n",
    "                n_xy_rs[(j,b_j,b_s)] = cnt\n",
    "                n_x_rs[(j,b_j)] += cnt\n",
    "                n_s_rs[b_s] += cnt\n",
    "            for j in candidates:\n",
    "                if (j,s) not in mi_red:\n",
    "                    I_js = sum((cnt/N) * math.log((cnt/N)/((n_x_rs[(j,b_j)]/N)*(n_s_rs[b_s]/N)))\n",
    "                               for ((jj,b_j,b_s),cnt) in n_xy_rs.items() if jj==j)\n",
    "                    mi_red[(j,s)] = mi_red[(s,j)] = I_js\n",
    "            best, best_score = None, float('-inf')\n",
    "            for j in candidates:\n",
    "                rel = mi_fd[j]\n",
    "                red_avg = sum(mi_red[(j,t)] for t in selected)/len(selected) if selected else 0.0\n",
    "                score = rel - red_avg\n",
    "                if score > best_score:\n",
    "                    best, best_score = j, score\n",
    "        if mi_fd[best] < Th:\n",
    "            break\n",
    "        selected.append(best)\n",
    "        candidates.remove(best)\n",
    "    print(f\"Final mRMRD subspace: {selected}\")\n",
    "\n",
    "    # 5) LOF computation\n",
    "    df_proj = df_norm.select(*selected, label_col).withColumn(\"id\", F.monotonically_increasing_id()).cache()\n",
    "    assembler2 = VectorAssembler(inputCols=selected, outputCol=\"features_vec\")\n",
    "    df_vec2 = assembler2.transform(df_proj).select(\"id\",\"features_vec\").cache()\n",
    "    lsh = BucketedRandomProjectionLSH(inputCol=\"features_vec\", outputCol=\"hashes\", bucketLength=math.sqrt(len(selected))/2)\n",
    "    model = lsh.fit(df_vec2)\n",
    "    max_dist = math.sqrt(len(selected))\n",
    "    pairs = (model.approxSimilarityJoin(df_vec2, df_vec2, max_dist, distCol=\"dist\")\n",
    "             .select(F.col(\"datasetA.id\").alias(\"pid\"), F.col(\"datasetB.id\").alias(\"oid\"), \"dist\")\n",
    "             .filter(\"pid < oid\"))\n",
    "    pairs = pairs.unionByName(pairs.selectExpr(\"oid as pid\",\"pid as oid\",\"dist\"))\n",
    "    w = F.row_number().over(Window.partitionBy(\"pid\").orderBy(\"dist\"))\n",
    "    knn = pairs.withColumn(\"rn\",w).filter(F.col(\"rn\")<=k).select(\"pid\",\"oid\",\"dist\").cache()\n",
    "    kdist = knn.groupBy(\"oid\").agg(F.max(\"dist\").alias(\"kdist\")).cache()\n",
    "    rd = knn.join(kdist, on=\"oid\").withColumn(\"reach_dist\",F.greatest(\"dist\",\"kdist\")).cache()\n",
    "    lrd = rd.groupBy(\"pid\").agg((F.lit(k)/F.sum(\"reach_dist\")).alias(\"lrd\")).cache()\n",
    "    lof = (rd.join(lrd.select(F.col(\"pid\").alias(\"oid\"),\"lrd\"),on=\"oid\")\n",
    "           .groupBy(\"pid\").agg(F.avg(\"lrd\").alias(\"avg_lrd_o\"))\n",
    "           .join(lrd,on=\"pid\")\n",
    "           .withColumn(\"LOF\",F.col(\"avg_lrd_o\")/F.col(\"lrd\")).cache())\n",
    "\n",
    "    # 6) AUC evaluation\n",
    "    score_label = (df_proj.join(lof.withColumnRenamed(\"pid\",\"id\").select(\"id\",\"LOF\"),on=\"id\")\n",
    "                   .select(\"LOF\", label_col)\n",
    "                   .rdd.map(lambda r: (float(r[0]), float(r[1]))))\n",
    "    metrics = BinaryClassificationMetrics(score_label)\n",
    "    auc = metrics.areaUnderROC\n",
    "\n",
    "    # Print results\n",
    "    t1 = time.time()\n",
    "    print(f\"Cores={cores:2d}  Time={(t1-t0):.1f}s  AUC={auc:.4f}\")\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
