{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\mkaze\\AppData\\Local\\Temp\\ipykernel_7076\\3892566662.py\", line 4, in <module>\n",
      "    from pyspark.ml.functions import vector_to_array\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\functions.py\", line 21, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\compat\\__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\compat\\pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\mkaze\\AppData\\Local\\Temp\\ipykernel_7076\\3892566662.py\", line 4, in <module>\n",
      "    from pyspark.ml.functions import vector_to_array\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\functions.py\", line 21, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row, Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from collections import defaultdict\n",
    "from operator import add\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jre1.8.0_441\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\Spark\\\\spark-3.5.5-bin-hadoop3\" \n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"outlier_dataset_500_50.csv\"\n",
    "nodes_list = [1, 2, 4, 8, 16]\n",
    "p = 2\n",
    "m_max = 5\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final mRMRD subspace: ['f1', 'f45', 'f38', 'f41', 'f36']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores= 1  Time=332.0s  AUC=0.8584\n",
      "Final mRMRD subspace: ['f1', 'f45', 'f38', 'f41', 'f36']\n",
      "Cores= 2  Time=211.3s  AUC=0.8584\n",
      "Final mRMRD subspace: ['f1', 'f45', 'f38', 'f41', 'f36']\n",
      "Cores= 4  Time=170.2s  AUC=0.8584\n",
      "Final mRMRD subspace: ['f1', 'f45', 'f38', 'f41', 'f36']\n",
      "Cores= 8  Time=171.9s  AUC=0.8584\n",
      "Final mRMRD subspace: ['f1', 'f45', 'f38', 'f41', 'f36']\n",
      "Cores=16  Time=202.1s  AUC=0.8584\n"
     ]
    }
   ],
   "source": [
    "# Loop over different simulated node counts\n",
    "for cores in nodes_list:\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Start Spark session with given number of local cores\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(f\"LOF_Eval_{cores}cores\") \\\n",
    "        .master(f\"local-cluster[{cores},2,4096]\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "        #.config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "\n",
    "    # 1) Load data and cache\n",
    "    df = spark.read.csv(data_path, header=True, inferSchema=True).cache()\n",
    "    cols = df.columns\n",
    "    label_col = cols[-1]\n",
    "    feature_cols = cols[:-1]\n",
    "\n",
    "    # 2) Normalize features to [0,1]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "    df_vec = assembler.transform(df).cache()\n",
    "    scaler = MinMaxScaler(inputCol=\"features_vec\", outputCol=\"scaled_vec\")\n",
    "    scaler_model = scaler.fit(df_vec)\n",
    "    df_scaled = scaler_model.transform(df_vec).cache()\n",
    "    df_scaled = df_scaled.withColumn(\"scaled_arr\", vector_to_array(\"scaled_vec\"))\n",
    "    scaled_exprs = [F.col(\"scaled_arr\")[i].alias(feature_cols[i]) for i in range(len(feature_cols))]\n",
    "    df_norm = df_scaled.select(*scaled_exprs, F.col(label_col)).cache()\n",
    "\n",
    "    # 3) Grid-based density via binning\n",
    "    width = 1.0 / p\n",
    "    def to_binned_row(row):\n",
    "        bins = [min(max(int(math.ceil(row[f] / width)), 1), p) for f in feature_cols]\n",
    "        rec = {f\"{feature_cols[i]}_bin\": bins[i] for i in range(len(feature_cols))}\n",
    "        rec[\"cube\"] = bins  # list of ints\n",
    "        return Row(**rec)\n",
    "    binned_rdd = df_norm.select(*feature_cols).rdd.map(to_binned_row)\n",
    "\n",
    "    # Explicit schema for binned DataFrame\n",
    "    bin_fields = [StructField(f\"{feature_cols[i]}_bin\", IntegerType(), False) for i in range(len(feature_cols))]\n",
    "    schema = StructType(bin_fields + [StructField(\"cube\", ArrayType(IntegerType(), containsNull=False), False)])\n",
    "    df_binned = spark.createDataFrame(binned_rdd, schema).cache()\n",
    "\n",
    "    # Count per cube (use tuple as key)\n",
    "    cube_count = df_binned.rdd.map(lambda r: (tuple(r[\"cube\"]), 1)).reduceByKey(add)\n",
    "    # Build density grid DataFrame\n",
    "    dg_schema = StructType([\n",
    "        StructField(\"cube\", ArrayType(IntegerType(), containsNull=False), False),\n",
    "        StructField(\"density\", IntegerType(), False)\n",
    "    ])\n",
    "    dg = (cube_count\n",
    "          .map(lambda x: (list(x[0]), x[1]))\n",
    "          .toDF(schema=dg_schema)\n",
    "          .cache())\n",
    "\n",
    "    # 4) mRMRD feature selection\n",
    "    bin_cols = [f\"{c}_bin\" for c in feature_cols]\n",
    "    df_dens = df_binned.join(dg, on=\"cube\").select(*bin_cols, \"density\").cache()\n",
    "    N = df_dens.count()\n",
    "\n",
    "    # Compute relevance MI(feature; density)\n",
    "    def rel_map(row):\n",
    "        y = row[\"density\"]\n",
    "        for f in feature_cols:\n",
    "            yield ((f, row[f\"{f}_bin\"], y), 1)\n",
    "    cont_rel = df_dens.rdd.flatMap(rel_map).reduceByKey(add).collect()\n",
    "    n_xy, n_x, n_y = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "    for (f, b, y), cnt in cont_rel:\n",
    "        n_xy[(f, b, y)] = cnt\n",
    "        n_x[(f, b)] += cnt\n",
    "        n_y[y] += cnt\n",
    "    mi_fd = {f: sum((cnt/N) * math.log((cnt/N)/((n_x[(f,b)]/N)*(n_y[y]/N)))\n",
    "                 for ((ff,b,y),cnt) in n_xy.items() if ff==f)\n",
    "             for f in feature_cols}\n",
    "    Th = sum(mi_fd.values()) / len(mi_fd)\n",
    "\n",
    "    selected, candidates, mi_red = [], feature_cols.copy(), {}\n",
    "    while candidates and len(selected) < m_max:\n",
    "        if not selected:\n",
    "            best = max(candidates, key=lambda x: mi_fd[x])\n",
    "        else:\n",
    "            s = selected[-1]\n",
    "            def red_map(row):\n",
    "                bs = row[f\"{s}_bin\"]\n",
    "                for j in candidates:\n",
    "                    yield ((j, row[f\"{j}_bin\"], bs), 1)\n",
    "            cont_rs = df_binned.rdd.flatMap(red_map).reduceByKey(add).collect()\n",
    "            n_xy_rs, n_x_rs, n_s_rs = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "            for (j, b_j, b_s), cnt in cont_rs:\n",
    "                n_xy_rs[(j,b_j,b_s)] = cnt\n",
    "                n_x_rs[(j,b_j)] += cnt\n",
    "                n_s_rs[b_s] += cnt\n",
    "            for j in candidates:\n",
    "                if (j,s) not in mi_red:\n",
    "                    I_js = sum((cnt/N) * math.log((cnt/N)/((n_x_rs[(j,b_j)]/N)*(n_s_rs[b_s]/N)))\n",
    "                               for ((jj,b_j,b_s),cnt) in n_xy_rs.items() if jj==j)\n",
    "                    mi_red[(j,s)] = mi_red[(s,j)] = I_js\n",
    "            best, best_score = None, float('-inf')\n",
    "            for j in candidates:\n",
    "                rel = mi_fd[j]\n",
    "                red_avg = sum(mi_red[(j,t)] for t in selected)/len(selected) if selected else 0.0\n",
    "                score = rel - red_avg\n",
    "                if score > best_score:\n",
    "                    best, best_score = j, score\n",
    "        if mi_fd[best] < Th:\n",
    "            break\n",
    "        selected.append(best)\n",
    "        candidates.remove(best)\n",
    "    print(f\"Final mRMRD subspace: {selected}\")\n",
    "\n",
    "    # 5) LOF computation\n",
    "    df_proj = df_norm.select(*selected, label_col).withColumn(\"id\", F.monotonically_increasing_id()).cache()\n",
    "    assembler2 = VectorAssembler(inputCols=selected, outputCol=\"features_vec\")\n",
    "    df_vec2 = assembler2.transform(df_proj).select(\"id\",\"features_vec\").cache()\n",
    "    lsh = BucketedRandomProjectionLSH(inputCol=\"features_vec\", outputCol=\"hashes\", bucketLength=math.sqrt(len(selected))/2)\n",
    "    model = lsh.fit(df_vec2)\n",
    "    max_dist = math.sqrt(len(selected))\n",
    "    pairs = (model.approxSimilarityJoin(df_vec2, df_vec2, max_dist, distCol=\"dist\")\n",
    "             .select(F.col(\"datasetA.id\").alias(\"pid\"), F.col(\"datasetB.id\").alias(\"oid\"), \"dist\")\n",
    "             .filter(\"pid < oid\"))\n",
    "    pairs = pairs.unionByName(pairs.selectExpr(\"oid as pid\",\"pid as oid\",\"dist\"))\n",
    "    w = F.row_number().over(Window.partitionBy(\"pid\").orderBy(\"dist\"))\n",
    "    knn = pairs.withColumn(\"rn\",w).filter(F.col(\"rn\")<=k).select(\"pid\",\"oid\",\"dist\").cache()\n",
    "    kdist = knn.groupBy(\"oid\").agg(F.max(\"dist\").alias(\"kdist\")).cache()\n",
    "    rd = knn.join(kdist, on=\"oid\").withColumn(\"reach_dist\",F.greatest(\"dist\",\"kdist\")).cache()\n",
    "    lrd = rd.groupBy(\"pid\").agg((F.lit(k)/F.sum(\"reach_dist\")).alias(\"lrd\")).cache()\n",
    "    lof = (rd.join(lrd.select(F.col(\"pid\").alias(\"oid\"),\"lrd\"),on=\"oid\")\n",
    "           .groupBy(\"pid\").agg(F.avg(\"lrd\").alias(\"avg_lrd_o\"))\n",
    "           .join(lrd,on=\"pid\")\n",
    "           .withColumn(\"LOF\",F.col(\"avg_lrd_o\")/F.col(\"lrd\")).cache())\n",
    "\n",
    "    t1 = time.time()\n",
    "    # 6) AUC evaluation\n",
    "    score_label = (df_proj.join(lof.withColumnRenamed(\"pid\",\"id\").select(\"id\",\"LOF\"),on=\"id\")\n",
    "                   .select(\"LOF\", label_col)\n",
    "                   .rdd.map(lambda r: (float(r[0]), float(r[1]))))\n",
    "    metrics = BinaryClassificationMetrics(score_label)\n",
    "    auc = metrics.areaUnderROC\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Cores={cores:2d}  Time={(t1-t0):.1f}s  AUC={auc:.4f}\")\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
