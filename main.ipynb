{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType, StructType, StructField\n",
    "from math import log\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jre1.8.0_441\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\Spark\\\\spark-3.5.5-bin-hadoop3\" \n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 0️⃣ Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "DATA_PATH      = \"all_data.csv\"      # → input file\n",
    "OUTPUT_PATH    = \"outlier_scores.csv\"\n",
    "\n",
    "P_BINS         = 10                   # equal‑width bins per feature for DDR\n",
    "M_FEATURES     = 5                    # number of features to keep (MRMRD)\n",
    "K_NEIGHBOURS   = 20                   # k for LOF\n",
    "SHUF_PARTS     = 200                  # shuffle partitions (set once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1️⃣ Spark Session\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"MRMRD‑LOF‑OutlierDetection\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", SHUF_PARTS)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2️⃣ Load & Min‑Max Normalise\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "df_raw = (spark.read\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .csv(DATA_PATH))\n",
    "\n",
    "label_col    = \"Label\"\n",
    "feature_cols = [c for c in df_raw.columns if c != label_col]\n",
    "\n",
    "# Compute min/max per feature once\n",
    "extrema = df_raw.agg(*([F.min(c).alias(f\"{c}_min\") for c in feature_cols] +\n",
    "                       [F.max(c).alias(f\"{c}_max\") for c in feature_cols]))\n",
    "mins = extrema.first().asDict()\n",
    "\n",
    "norm_exprs = []\n",
    "for c in feature_cols:\n",
    "    denom = mins[f\"{c}_max\"] - mins[f\"{c}_min\"] or 1.0\n",
    "    norm_exprs.append(((F.col(c) - F.lit(mins[f\"{c}_min\"])) / F.lit(denom)).alias(f\"{c}_norm\"))\n",
    "\n",
    "df_norm = df_raw.select(norm_exprs + [label_col])\n",
    "norm_cols = [f\"{c}_norm\" for c in feature_cols]\n",
    "\n",
    "# Cache because we access it twice (DDR + LOF)\n",
    "_ = df_norm.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3️⃣ Fast Density‑Based Representation (DDR)\n",
    "# ---------------------------------------------------------------------------\n",
    "#   • one Catalyst projection produces all bin indices\n",
    "#   • cube‑key built as a compact string → smaller shuffle\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "bin_cols    = [f\"{c}_bin\" for c in norm_cols]\n",
    "\n",
    "bin_exprs = [\n",
    "    F.least(F.floor(F.col(c) * P_BINS).cast(IntegerType()) + 1, F.lit(P_BINS)).alias(bc)\n",
    "    for c, bc in zip(norm_cols, bin_cols)\n",
    "]\n",
    "\n",
    "key_col = \"cube_key\"\n",
    "\n",
    "# Select original normalised features, label, and freshly built bins\n",
    "# Building the key as concat_ws('#', …) keeps it narrow for shuffles\n",
    "\n",
    "df_bins = (df_norm\n",
    "             .select([F.col(label_col)] + norm_cols + bin_exprs)\n",
    "             .withColumn(key_col, F.concat_ws('#', *[F.col(bc) for bc in bin_cols])))\n",
    "\n",
    "# MapReduce: cube_key → density\n",
    "\n",
    "densities = (df_bins.groupBy(key_col)\n",
    "                     .count()\n",
    "                     .withColumnRenamed(\"count\", \"density\"))\n",
    "\n",
    "with_density = df_bins.join(densities, key_col, \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 4️⃣ Mutual Information helpers (histogram‑based)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def compute_mutual_information(df_in, x_col, y_col):\n",
    "    \"\"\"Entropy‑based MI between two *discrete* integer columns.\"\"\"\n",
    "    n = df_in.count()\n",
    "\n",
    "    joint = df_in.groupBy(x_col, y_col).count()\n",
    "    px    = df_in.groupBy(x_col).count().withColumnRenamed(\"count\", \"cx\")\n",
    "    py    = df_in.groupBy(y_col).count().withColumnRenamed(\"count\", \"cy\")\n",
    "\n",
    "    joined = (joint.join(px, x_col).join(py, y_col))\n",
    "\n",
    "    mi = (joined\n",
    "            .withColumn(\"term\",\n",
    "                        (F.col(\"count\")/n) *\n",
    "                        F.log((F.col(\"count\")/n) /\n",
    "                              ((F.col(\"cx\")/n) * (F.col(\"cy\")/n))))\n",
    "            .agg(F.sum(\"term\").alias(\"mi\")).first()[\"mi\"])\n",
    "    return mi or 0.0\n",
    "\n",
    "with_density.cache()\n",
    "\n",
    "density_col = \"density\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 5‑1: relevance for every candidate\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bin_cols:\n\u001b[1;32m---> 11\u001b[0m     mi_feature_density[bc] \u001b[38;5;241m=\u001b[39m compute_mutual_information(with_density, bc, density_col)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 5‑2: pick the most relevant first\u001b[39;00m\n\u001b[0;32m     14\u001b[0m selected\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mmax\u001b[39m(mi_feature_density, key\u001b[38;5;241m=\u001b[39mmi_feature_density\u001b[38;5;241m.\u001b[39mget))\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mcompute_mutual_information\u001b[1;34m(df_in, x_col, y_col)\u001b[0m\n\u001b[0;32m     11\u001b[0m py    \u001b[38;5;241m=\u001b[39m df_in\u001b[38;5;241m.\u001b[39mgroupBy(y_col)\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m joined \u001b[38;5;241m=\u001b[39m (joint\u001b[38;5;241m.\u001b[39mjoin(px, x_col)\u001b[38;5;241m.\u001b[39mjoin(py, y_col))\n\u001b[0;32m     15\u001b[0m mi \u001b[38;5;241m=\u001b[39m (joined\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m                     (F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m/\u001b[39mn) \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     18\u001b[0m                     F\u001b[38;5;241m.\u001b[39mlog((F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m/\u001b[39mn) \u001b[38;5;241m/\u001b[39m\n\u001b[0;32m     19\u001b[0m                           ((F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m/\u001b[39mn) \u001b[38;5;241m*\u001b[39m (F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m/\u001b[39mn))))\n\u001b[1;32m---> 20\u001b[0m         \u001b[38;5;241m.\u001b[39magg(F\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmi\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mfirst()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmi\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mi \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:2997\u001b[0m, in \u001b[0;36mDataFrame.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Row]:\n\u001b[0;32m   2978\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the first row as a :class:`Row`.\u001b[39;00m\n\u001b[0;32m   2979\u001b[0m \n\u001b[0;32m   2980\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2995\u001b[0m \u001b[38;5;124;03m    Row(age=2, name='Alice')\u001b[39;00m\n\u001b[0;32m   2996\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:2973\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   2941\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the first ``n`` rows.\u001b[39;00m\n\u001b[0;32m   2942\u001b[0m \n\u001b[0;32m   2943\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2970\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice')]\u001b[39;00m\n\u001b[0;32m   2971\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2973\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2975\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(n)\n",
      "File \u001b[1;32mc:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:2975\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   2973\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2975\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(n)\n",
      "File \u001b[1;32mc:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1407\u001b[0m, in \u001b[0;36mDataFrame.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \n\u001b[0;32m   1381\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;124;03m    [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\u001b[39;00m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimit(num)\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mc:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m \n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32mc:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mkaze\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 5️⃣ MRMRD Feature Selection\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "selected             = []\n",
    "mi_feature_density    = {}\n",
    "mi_pair_cache        = {}\n",
    "\n",
    "# 5‑1: relevance for every candidate\n",
    "for bc in bin_cols:\n",
    "    mi_feature_density[bc] = compute_mutual_information(with_density, bc, density_col)\n",
    "\n",
    "# 5‑2: pick the most relevant first\n",
    "selected.append(max(mi_feature_density, key=mi_feature_density.get))\n",
    "\n",
    "# 5‑3: iterative selection\n",
    "while len(selected) < M_FEATURES:\n",
    "    best_feat, best_score = None, float('-inf')\n",
    "    for cand in bin_cols:\n",
    "        if cand in selected:\n",
    "            continue\n",
    "        rel = mi_feature_density[cand]\n",
    "        red = 0.0\n",
    "        for s in selected:\n",
    "            key = tuple(sorted((cand, s)))\n",
    "            if key not in mi_pair_cache:\n",
    "                mi_pair_cache[key] = compute_mutual_information(with_density, cand, s)\n",
    "            red += mi_pair_cache[key]\n",
    "        red /= len(selected)\n",
    "        score = rel - red\n",
    "        if score > best_score:\n",
    "            best_feat, best_score = cand, score\n",
    "    selected.append(best_feat)\n",
    "\n",
    "print(\"[MRMRD] Selected bins:\", selected)\n",
    "selected_norm = [c.replace(\"_bin\", \"\") for c in selected]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 6️⃣ LOF in Selected Subspace (simple partition‑local version)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import Row\n",
    "\n",
    "vec_col   = \"features_vec\"\n",
    "assembler = VectorAssembler(inputCols=selected_norm, outputCol=vec_col)\n",
    "sub_df    = assembler.transform(with_density.select(selected_norm + [label_col]))\n",
    "\n",
    "sub_rdd = sub_df.select(vec_col, label_col).rdd.repartition(SHUF_PARTS).cache()\n",
    "\n",
    "k = K_NEIGHBOURS\n",
    "\n",
    "def lof_partition(iter_rows):\n",
    "    pts = list(iter_rows)\n",
    "    if not pts:\n",
    "        return\n",
    "    vecs   = np.array([r[0] for r in pts])\n",
    "    labels = [r[1] for r in pts]\n",
    "    n      = len(vecs)\n",
    "\n",
    "    dists  = np.linalg.norm(vecs[:, None, :] - vecs[None, :, :], axis=2)\n",
    "    k_dist = np.partition(dists, k, axis=1)[:, k]\n",
    "    reach  = np.maximum(k_dist[None, :], dists)\n",
    "    lrd    = k / np.sum(np.partition(reach, k, axis=1)[:, :k], axis=1)\n",
    "    lof    = np.sum(lrd[None, :] / lrd[:, None] * (dists <= k_dist[None, :]), axis=1) / k\n",
    "\n",
    "    for lbl, score in zip(labels, lof):\n",
    "        yield Row(Label=lbl, LOF_Score=float(score))\n",
    "\n",
    "lof_scores = sub_rdd.mapPartitions(lof_partition).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 7️⃣ Output\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "(lof_scores\n",
    "   .write\n",
    "   .option(\"header\", True)\n",
    "   .mode(\"overwrite\")\n",
    "   .csv(OUTPUT_PATH))\n",
    "\n",
    "print(f\"[✔] LOF scores written to {OUTPUT_PATH}\")\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
