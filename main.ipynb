{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from operator import add\n",
    "import numpy as np\n",
    "from math import log\n",
    "from itertools import combinations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jre1.8.0_441\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\Spark\\\\spark-3.5.5-bin-hadoop3\" \n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 0️⃣ Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "DATA_PATH      = \"all_data.csv\"      # input file\n",
    "OUTPUT_PATH    = \"outlier_scores.csv\"\n",
    "\n",
    "P_BINS         = 10                   # grid bins per feature (DDR)\n",
    "M_FEATURES     = 5                    # features to keep (MRMRD)\n",
    "K_NEIGHBOURS   = 20                   # k for LOF\n",
    "SHUF_PARTS     = 200                  # global shuffle partition setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1️⃣ Spark Session\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"MRMRD‑LOF‑OutlierDetection\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", SHUF_PARTS)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2️⃣ Load & Min‑Max Normalise\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "df_raw = (spark.read\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .csv(DATA_PATH))\n",
    "\n",
    "label_col    = \"Label\"\n",
    "feature_cols = [c for c in df_raw.columns if c != label_col]\n",
    "\n",
    "# Compute min/max per feature once\n",
    "extrema = df_raw.agg(*([F.min(c).alias(f\"{c}_min\") for c in feature_cols] +\n",
    "                       [F.max(c).alias(f\"{c}_max\") for c in feature_cols]))\n",
    "mins = extrema.first().asDict()\n",
    "\n",
    "norm_exprs = []\n",
    "for c in feature_cols:\n",
    "    denom = mins[f\"{c}_max\"] - mins[f\"{c}_min\"] or 1.0\n",
    "    norm_exprs.append(((F.col(c) - F.lit(mins[f\"{c}_min\"])) / F.lit(denom)).alias(f\"{c}_norm\"))\n",
    "\n",
    "df_norm = df_raw.select(norm_exprs + [label_col])\n",
    "\n",
    "norm_cols = [f\"{c}_norm\" for c in feature_cols]\n",
    "\n",
    "_ = df_norm.cache().count()  # cache for DDR & LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3️⃣ Fast Density‑Based Representation (DDR)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "bin_cols = [f\"{c}_bin\" for c in norm_cols]\n",
    "\n",
    "bin_exprs = [\n",
    "    F.least(F.floor(F.col(c) * P_BINS).cast(IntegerType()) + 1, F.lit(P_BINS)).alias(bc)\n",
    "    for c, bc in zip(norm_cols, bin_cols)\n",
    "]\n",
    "\n",
    "key_col = \"cube_key\"\n",
    "\n",
    "df_bins = (df_norm\n",
    "             .select([F.col(label_col)] + norm_cols + bin_exprs)\n",
    "             .withColumn(key_col, F.concat_ws('#', *[F.col(bc) for bc in bin_cols])))\n",
    "\n",
    "densities = (df_bins.groupBy(key_col)\n",
    "                     .count()\n",
    "                     .withColumnRenamed(\"count\", \"density\"))\n",
    "\n",
    "with_density = df_bins.join(densities, key_col, \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 4️⃣ Global statistics for MI(feature; density)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "density_col = \"density\"\n",
    "num_feats   = len(bin_cols)\n",
    "\n",
    "rdd_joint = (with_density\n",
    "               .select(bin_cols + [density_col])\n",
    "               .rdd\n",
    "               .flatMap(lambda row: [((i, int(row[i]), int(row[-1])), 1) for i in range(num_feats)]))\n",
    "\n",
    "joint_counts = dict(rdd_joint.reduceByKey(add).collect())\n",
    "\n",
    "# marginals\n",
    "p_y = {}\n",
    "for (_, _, y), c in joint_counts.items():\n",
    "    p_y[y] = p_y.get(y, 0) + c\n",
    "\n",
    "total_n = float(sum(p_y.values()))\n",
    "for y in p_y:\n",
    "    p_y[y] /= total_n\n",
    "\n",
    "p_x = [{} for _ in range(num_feats)]\n",
    "for (f, x, _), c in joint_counts.items():\n",
    "    p_x[f][x] = p_x[f].get(x, 0) + c\n",
    "for f in range(num_feats):\n",
    "    for x in p_x[f]:\n",
    "        p_x[f][x] /= total_n\n",
    "\n",
    "mi_feature_density = {bc: 0.0 for bc in bin_cols}\n",
    "for (f, x, y), c in joint_counts.items():\n",
    "    bc = bin_cols[f]\n",
    "    pxy = c / total_n\n",
    "    mi_feature_density[bc] += pxy * log(pxy / (p_x[f][x] * p_y[y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MRMRD] first feature → _c18_norm_bin\n",
      "[MRMRD] add → _c463_norm_bin (score=-0.0119)\n",
      "[MRMRD] add → _c480_norm_bin (score=-0.0119)\n",
      "[MRMRD] add → _c336_norm_bin (score=-0.0119)\n",
      "[MRMRD] add → _c360_norm_bin (score=-0.0119)\n",
      "[MRMRD] Selected bins: ['_c18_norm_bin', '_c463_norm_bin', '_c480_norm_bin', '_c336_norm_bin', '_c360_norm_bin']\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 5️⃣ MRMRD Selection – 1 Spark pass per new feature\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "selected  = []\n",
    "pair_mi   = {}\n",
    "\n",
    "first_feat = max(mi_feature_density, key=mi_feature_density.get)\n",
    "selected.append(first_feat)\n",
    "print(\"[MRMRD] first feature →\", first_feat)\n",
    "\n",
    "def compute_mi_against_selected(sel_feat):\n",
    "    \"\"\"Compute MI(sel_feat, cand) for every remaining candidate via a single RDD pass.\"\"\"\n",
    "    rem_feats = [c for c in bin_cols if c not in selected]\n",
    "    if not rem_feats:\n",
    "        return\n",
    "\n",
    "    # Select columns in deterministic order: sel + rem_feats\n",
    "    cols = [sel_feat] + rem_feats\n",
    "\n",
    "    def mapper(row):\n",
    "        s_val = int(row[0])\n",
    "        for j, cand_feat in enumerate(rem_feats):\n",
    "            c_val = int(row[j+1])\n",
    "            yield ((sel_feat, cand_feat, s_val, c_val), 1)\n",
    "\n",
    "    joint_rdd = (with_density.select(cols)\n",
    "                               .rdd\n",
    "                               .flatMap(mapper)\n",
    "                               .reduceByKey(add))\n",
    "\n",
    "    for (s_feat, cand_feat, s_val, c_val), c_xy in joint_rdd.collect():\n",
    "        sel_idx  = bin_cols.index(s_feat)\n",
    "        cand_idx = bin_cols.index(cand_feat)\n",
    "        pa = p_x[sel_idx][s_val]\n",
    "        pb = p_x[cand_idx][c_val]\n",
    "        pxy = c_xy / total_n\n",
    "        mi_val = pxy * log(pxy / (pa * pb))\n",
    "        pair_mi.setdefault((s_feat, cand_feat), 0.0)\n",
    "        pair_mi[(s_feat, cand_feat)] += mi_val\n",
    "        pair_mi[(cand_feat, s_feat)] = pair_mi[(s_feat, cand_feat)]\n",
    "\n",
    "compute_mi_against_selected(first_feat)\n",
    "\n",
    "while len(selected) < M_FEATURES:\n",
    "    best_feat, best_score = None, float('-inf')\n",
    "    for cand in bin_cols:\n",
    "        if cand in selected:\n",
    "            continue\n",
    "        rel = mi_feature_density[cand]\n",
    "        red = sum(pair_mi.get((cand, s), 0.0) for s in selected) / len(selected)\n",
    "        score = rel - red\n",
    "        if score > best_score:\n",
    "            best_feat, best_score = cand, score\n",
    "    selected.append(best_feat)\n",
    "    print(f\"[MRMRD] add → {best_feat} (score={best_score:.4f})\")\n",
    "    compute_mi_against_selected(best_feat)\n",
    "\n",
    "print(\"[MRMRD] Selected bins:\", selected)\n",
    "selected_norm = [c.replace(\"_bin\", \"\") for c in selected]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 6️⃣ LOF in the selected sub‑space (distributed, robust v8)\n",
    "# ---------------------------------------------------------------------------\n",
    "# (after `selected_norm` is ready)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "from math import inf\n",
    "\n",
    "K_NEIGH = 20  # global default; can be overridden via --conf or arg\n",
    "\n",
    "lofschema = StructType([\n",
    "    StructField(\"Label\",      StringType(),  True),\n",
    "    StructField(\"LOF_Score\",  DoubleType(),  True),\n",
    "])\n",
    "\n",
    "sub_df  = with_density.select(\"Label\", *selected_norm)  # keep only needed cols\n",
    "sub_rdd = sub_df.rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\mkaze\\AppData\\Local\\Temp\\ipykernel_15436\\3846632051.py\", line 60, in <module>\n",
      "    lof_scores = spark.createDataFrame(lof_rdd, schema=lofschema)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\session.py\", line 1397, in createDataFrame\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\compat\\__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\compat\\pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\mkaze\\AppData\\Local\\Temp\\ipykernel_15436\\3846632051.py\", line 60, in <module>\n",
      "    lof_scores = spark.createDataFrame(lof_rdd, schema=lofschema)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\session.py\", line 1397, in createDataFrame\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# partition‑local LOF helper – numpy only, handles 1‑D feature case & empty dims\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def lof_partition(iter_rows):\n",
    "    import numpy as np\n",
    "\n",
    "    rows   = list(iter_rows)\n",
    "    n      = len(rows)\n",
    "\n",
    "    if n == 0:\n",
    "        return  # empty partition – nothing to yield\n",
    "\n",
    "    labels = [r[0] for r in rows]\n",
    "    pts    = np.asarray([r[1:] for r in rows], dtype=float)\n",
    "\n",
    "    # ensure 2‑D even if only 1 feature\n",
    "    if pts.ndim == 1:\n",
    "        pts = pts[:, None]\n",
    "\n",
    "    m = pts.shape[1]\n",
    "    if m == 0:\n",
    "        # no features selected – cannot compute LOF; emit NaNs\n",
    "        for lbl in labels:\n",
    "            yield (lbl, float('nan'))\n",
    "        return\n",
    "\n",
    "    if n < 2:\n",
    "        for lbl in labels:\n",
    "            yield (lbl, float('nan'))\n",
    "        return\n",
    "\n",
    "    k = min(K_NEIGH, n - 1)\n",
    "\n",
    "    # pair‑wise squared Euclidean distances\n",
    "    dmat = np.sum((pts[:, None, :] - pts[None, :, :]) ** 2, axis=2)\n",
    "    np.fill_diagonal(dmat, np.inf)\n",
    "\n",
    "    # k‑distance of each point\n",
    "    idx = np.argpartition(dmat, k, axis=1)[:, :k]\n",
    "    k_dist = np.take_along_axis(dmat, idx, axis=1).max(axis=1)  # size n\n",
    "\n",
    "    # reachability distances\n",
    "    reach_d = np.maximum(dmat, k_dist[:, None])\n",
    "\n",
    "    # local reachability density\n",
    "    lrd = k / np.take_along_axis(reach_d, idx, axis=1).sum(axis=1)\n",
    "\n",
    "    # LOF\n",
    "    lrd_ratio_sum = (lrd[idx] / lrd[:, None]).sum(axis=1)\n",
    "    lof_scores = lrd_ratio_sum / k\n",
    "\n",
    "    for lbl, score in zip(labels, lof_scores):\n",
    "        yield (lbl, float(score))\n",
    "\n",
    "# run partition function\n",
    "lof_rdd = sub_rdd.mapPartitions(lof_partition)\n",
    "\n",
    "# materialise DataFrame with predefined schema\n",
    "lof_scores = spark.createDataFrame(lof_rdd, schema=lofschema)\n",
    "lof_scores = spark.createDataFrame(lof_rdd, schema=lofschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Label: string, LOF_Score: double]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lof_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\mkaze\\AppData\\Local\\Temp\\ipykernel_15436\\3020450338.py\", line 35, in <module>\n",
      "    lof_scores.toPandas().to_csv(FINAL_CSV, index=False)\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py\", line 98, in toPandas\n",
      "    require_minimum_pyarrow_version()\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\pandas\\utils.py\", line 53, in require_minimum_pyarrow_version\n",
      "    import pyarrow\n",
      "  File \"c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mkaze\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✔] LOF scores written via Pandas to outlier_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 7️⃣ Output – robust cross‑platform write\n",
    "# ---------------------------------------------------------------------------\n",
    "# Native Hadoop I/O on Windows may still blow up even after disabling NativeIO.\n",
    "# So we attempt the Spark CSV write first; if that fails we fall back to a\n",
    "# driver‑side Pandas write. Either way you get a single `outlier_scores.csv`.\n",
    "\n",
    "#OUTPUT_DIR = \"outlier_scores\"  # will contain the final .csv or part‑file\n",
    "#TEMP_DIR   = OUTPUT_DIR + \"_spark\"  # Spark will write here first\n",
    "FINAL_CSV  = os.path.join(\"outlier_scores.csv\")\n",
    "\n",
    "import os, shutil, glob\n",
    "#from pyspark.sql.utils import Py4JJavaError\n",
    "\n",
    "# Spark setting that often avoids extra directory scans\n",
    "spark.conf.set(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "#try:\n",
    "#    # 1️⃣ Let Spark write to a temp directory (single part‑file)\n",
    "#    (lof_scores\n",
    "#        .coalesce(1)  # produce one part file for easy rename\n",
    "#        .write\n",
    "#        .option(\"header\", True)\n",
    "#        .mode(\"overwrite\")\n",
    "#        .csv(TEMP_DIR))\n",
    "#\n",
    "#    # 2️⃣ Move/rename the part‑file to our final CSV path\n",
    "#    part_file = glob.glob(os.path.join(TEMP_DIR, \"part-*\"))[0]\n",
    "#    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "#    shutil.move(part_file, FINAL_CSV)\n",
    "#    shutil.rmtree(TEMP_DIR)\n",
    "#    print(f\"[✔] LOF scores written to {FINAL_CSV}\")\n",
    "\n",
    "# Pandas fallback (collect to driver)\n",
    "lof_scores.toPandas().to_csv(FINAL_CSV, index=False)\n",
    "print(f\"[✔] LOF scores written via Pandas to {FINAL_CSV}\")\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
