{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T16:55:50.371636Z",
     "start_time": "2025-07-05T16:55:50.368236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession, functions as F, Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from operator import add\n",
    "import numpy as np\n",
    "from math import log\n",
    "import logging, os\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import math\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import heapq"
   ],
   "id": "3e764583eef5453e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:30:27.657281Z",
     "start_time": "2025-07-05T09:30:26.480007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Dimension Reduction\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ],
   "id": "4933f9a1827ee965",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Import Musk version 2 dataset",
   "id": "c222974093049c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:31:43.993202Z",
     "start_time": "2025-07-05T09:30:53.040096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# fetch dataset\n",
    "musk_version_2 = fetch_ucirepo(id=75)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = musk_version_2.data.features\n",
    "y = musk_version_2.data.targets\n",
    "\n",
    "pdf = pd.concat([X, y], axis=1)\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# grab column names\n",
    "label_col = y.columns[0] if hasattr(y, \"columns\") else \"class\"\n",
    "feature_cols = [c for c in df.columns if c != label_col]"
   ],
   "id": "1fec2d6869cd90e5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Scale With Max_Min Normalization method",
   "id": "23748e5082f48ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:32:00.499893Z",
     "start_time": "2025-07-05T09:31:55.559189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "assembled_df = assembler.transform(df)\n",
    "\n",
    "# apply spark built-in min-max scaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "df_scaled = scaler_model.transform(assembled_df)\n",
    "\n"
   ],
   "id": "9879cdf7d463a251",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/05 13:01:57 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Density Based Representation",
   "id": "45868de071e2d9c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T10:30:13.078323Z",
     "start_time": "2025-07-05T10:30:12.881597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# number of bins per feature\n",
    "p = 10\n",
    "\n",
    "cube_counts = (\n",
    "    df_scaled.select(\"scaledFeatures\").rdd\n",
    "    # MAP: vector → tuple of bin indices\n",
    "    .map(lambda row: tuple(int(min(x * p, p - 1)) for x in row.scaledFeatures))\n",
    "    # MAP: tuple → (cube_id, 1)\n",
    "    .map(lambda bins: (\"_\".join(map(str, bins)), 1))\n",
    "    # REDUCE: sum counts per cube_id\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n"
   ],
   "id": "68bd1ed668bb3092",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T10:30:17.694243Z",
     "start_time": "2025-07-05T10:30:13.803715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_feats = len(feature_cols)\n",
    "\n",
    "density_df = spark.createDataFrame(\n",
    "    cube_counts.map(lambda kv: Row(cube_id=kv[0], density=kv[1]))\n",
    ")\n",
    "\n",
    "for i in range(num_feats):\n",
    "    density_df = density_df.withColumn(\n",
    "        f\"g{i}\", F.split(F.col(\"cube_id\"), \"_\")[i].cast(\"int\")\n",
    "    )\n"
   ],
   "id": "502212e53784215e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## mRMD-Based Relevant Subspace Selection",
   "id": "f26e159fe869b7eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T10:30:21.515400Z",
     "start_time": "2025-07-05T10:30:18.591980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RDD format\n",
    "col_names = [f\"g{i}\" for i in range(num_feats)]\n",
    "mr_rdd    = density_df.select(col_names + [\"density\"]).rdd.cache()\n",
    "N_total   = mr_rdd.count()\n",
    "\n",
    "# calculate similarity\n",
    "fd_counts = (\n",
    "    mr_rdd.flatMap(\n",
    "        lambda row: [((j, getattr(row, col_names[j]), row.density), 1) for j in range(num_feats)]\n",
    "    ).reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n",
    "feat_marg = fd_counts.map(lambda kv: ((kv[0][0], kv[0][1]), kv[1])).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "dens_marg = fd_counts.map(lambda kv: (kv[0][2], kv[1])).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "fd_list      = fd_counts.collect()\n",
    "feat_dict    = dict(feat_marg.collect())\n",
    "dens_dict    = dict(dens_marg.collect())\n",
    "\n",
    "mi_relevance = {}\n",
    "for (j, gval, dc), cnt in fd_list:\n",
    "    p_joint = cnt / N_total\n",
    "    p_g     = feat_dict[(j, gval)] / N_total\n",
    "    p_d     = dens_dict[dc] / N_total\n",
    "    mi_relevance[j] = mi_relevance.get(j, 0.0) + p_joint * math.log2(p_joint / (p_g * p_d))\n"
   ],
   "id": "7900ba1abfd3cda7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Compute I(gi,gj) And Redundancy",
   "id": "ffbc710b43d0fd91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T10:54:07.581686Z",
     "start_time": "2025-07-05T10:53:29.035154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pair_counts = (\n",
    "    mr_rdd.flatMap(\n",
    "        lambda row: [(((j, l, getattr(row, col_names[j]), getattr(row, col_names[l]))), 1)\n",
    "                      for j in range(num_feats) for l in range(j + 1, num_feats)]\n",
    "    ).reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n",
    "# aggregate pair dictionaries ( (j , l) , ( (v1,v2),cnt))\n",
    "from collections import defaultdict\n",
    "pair_dict = defaultdict(list)\n",
    "for ((j, l, vj, vl), c) in pair_counts.collect():\n",
    "    pair_dict[(j, l)].append(((vj, vl), c))\n",
    "\n",
    "# compute mutual information\n",
    "mi_pair = {}\n",
    "for (j, l), items in pair_dict.items():\n",
    "    score = 0.0\n",
    "    for (vj, vl), cnt in items:\n",
    "        p_joint = cnt / N_total\n",
    "        p_j     = feat_dict[(j, vj)] / N_total\n",
    "        p_l     = feat_dict[(l, vl)] / N_total\n",
    "        score  += p_joint * math.log2(p_joint / (p_j * p_l))\n",
    "    mi_pair[(j, l)] = score\n",
    "    mi_pair[(l, j)] = score"
   ],
   "id": "1b3176127ccff12",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Greedy mRMD Selection",
   "id": "465560a1f3d1f9fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T11:31:57.043028Z",
     "start_time": "2025-07-05T11:31:57.038010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# select the number of desired subspace\n",
    "subspace_size = 10\n",
    "selected, remaining = [], list(range(num_feats))\n",
    "\n",
    "while remaining and len(selected) < subspace_size:\n",
    "    best, best_score = None, float(\"-inf\")\n",
    "    for cand in remaining:\n",
    "        redund = 0.0\n",
    "        if selected:\n",
    "            redund = sum(mi_pair.get((cand, s), 0.0) for s in selected) / len(selected)\n",
    "        score = mi_relevance[cand] - redund\n",
    "        if score > best_score:\n",
    "            best, best_score = cand, score\n",
    "    selected.append(best)\n",
    "    remaining.remove(best)\n",
    "\n",
    "# see the selected features\n",
    "print(\"\\nSelected features (MRMD order):\", [f\"g{i}\" for i in selected])"
   ],
   "id": "82600542849d40d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features (MRMD order): ['g44', 'g4', 'g146', 'g101', 'g156', 'g145', 'g93', 'g144', 'g66', 'g110']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stage 5: Data Mapping",
   "id": "70fdab05d2af9fb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T16:44:38.524951Z",
     "start_time": "2025-07-05T16:44:35.320507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Convert 'gXX' to integer indices and broadcast for tiny closures\n",
    "indexes = [\n",
    "    col if isinstance(col, int) else int(col[1:])   # 'g12' → 12, 12 → 12\n",
    "    for col in selected\n",
    "]\n",
    "bc_idx = spark.sparkContext.broadcast(indexes)\n",
    "\n",
    "# Add a row-id so we can join later if needed\n",
    "norm_rdd = (\n",
    "    df_scaled.rdd\n",
    "            .zipWithIndex()\n",
    "            .map(lambda t: (t[1], t[0].scaledFeatures))\n",
    ")\n",
    "\n",
    "proj_rdd = (\n",
    "    norm_rdd\n",
    "      .map(lambda kv: (kv[0],\n",
    "                       [kv[1][i] for i in bc_idx.value]))\n",
    ")\n",
    "\n",
    "\n",
    "subspace_df = (\n",
    "    proj_rdd\n",
    "      .map(lambda kv: (kv[0], Vectors.dense(kv[1])))\n",
    "      .toDF([\"id\", \"subspaceFeatures\"])\n",
    ")\n",
    "\n",
    "subspace_df.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "print(f\"✓ Data-Mapping complete – projected into {len(indexes)}-D sub-space.\")\n",
    "subspace_df.show(5, truncate=False)\n"
   ],
   "id": "88d1cc827c79c322",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data-Mapping complete – projected into 10-D sub-space.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |subspaceFeatures                                                                                                                                                                                     |\n",
      "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |[0.061269146608315096,0.002257336343115124,0.06493506493506494,0.47506561679790027,0.03233830845771144,0.013623978201634877,0.03125,0.00510204081632653,0.0024937655860349127,0.02396514161220044]   |\n",
      "|1  |[0.15098468271334792,0.002257336343115124,0.048701298701298704,0.4461942257217848,0.05472636815920398,0.010899182561307902,0.443359375,0.00510204081632653,0.0024937655860349127,0.04139433551198257]|\n",
      "|2  |[0.1487964989059081,0.002257336343115124,0.0551948051948052,0.4068241469816273,0.0472636815920398,0.010899182561307902,0.474609375,0.00510204081632653,0.0024937655860349127,0.04357298474945534]    |\n",
      "|3  |[0.1487964989059081,0.002257336343115124,0.048701298701298704,0.4461942257217848,0.05223880597014925,0.013623978201634877,0.443359375,0.00510204081632653,0.0024937655860349127,0.04139433551198257] |\n",
      "|4  |[0.1487964989059081,0.002257336343115124,0.048701298701298704,0.4461942257217848,0.05223880597014925,0.013623978201634877,0.443359375,0.00510204081632653,0.0024937655860349127,0.04139433551198257] |\n",
      "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Stage 6: Compute LOF Scores\n",
    "#### First Find K Nearest Neighbors"
   ],
   "id": "143aa64fc4244767"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T16:58:07.832217Z",
     "start_time": "2025-07-05T16:58:07.801845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k = 50\n",
    "rdd = subspace_df.rdd.map(lambda r: (r.id, r.subspaceFeatures))\n",
    "\n",
    "pairs = (\n",
    "    rdd.cartesian(rdd)\n",
    "        .filter(lambda t: t[0][0] != t[1][0])\n",
    "        .map(lambda t: ( t[0][0],\n",
    "                         ( Vectors.squared_distance(t[0][1], t[1][1]),\n",
    "                           t[1][0]) ))\n",
    ")\n",
    "\n",
    "def top_k(acc, x):\n",
    "    if len(acc) < k:\n",
    "        heapq.heappush(acc, (-x[0], x[1]))           # max-heap keeps k smallest\n",
    "    else:\n",
    "        heapq.heappushpop(acc, (-x[0], x[1]))\n",
    "    return acc\n",
    "\n",
    "knn = pairs.aggregateByKey([], top_k,\n",
    "                           lambda a, b: heapq.nsmallest(k, a + b, key=lambda t: -t[0]))\n"
   ],
   "id": "ab675b9e33094fc0",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Then Compute Local Reachability Density",
   "id": "85aa66d63467ed52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:03:13.436871Z",
     "start_time": "2025-07-05T17:02:25.215310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kdist = knn.mapValues(lambda lst: max(-d2 for d2, _ in lst)).collectAsMap()\n",
    "bc_kd  = spark.sparkContext.broadcast(kdist)\n",
    "\n",
    "reach_rdd = knn.flatMap(\n",
    "    lambda item: [\n",
    "        (item[0], (max(-d2, bc_kd.value[j]), 1.0))\n",
    "        for d2, j in item[1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "lrd = (reach_rdd\n",
    "       .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "       .mapValues(lambda s: s[1] / s[0]))\n"
   ],
   "id": "eec61e60c185b6ff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### LOF(i) = Σ LRD(j) / (k * LRD(i)",
   "id": "ff1dc8514fc377fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:06:25.022313Z",
     "start_time": "2025-07-05T17:06:22.701849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bc_lrd = spark.sparkContext.broadcast(lrd.collectAsMap())\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\",   LongType(),  False),\n",
    "    StructField(\"lof\",  DoubleType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "lof_rdd = knn.map(\n",
    "    lambda item: (\n",
    "        item[0],\n",
    "        sum(bc_lrd.value[j] for _, j in item[1]) / (k * bc_lrd.value[item[0]])\n",
    "    )\n",
    ")\n",
    "\n",
    "lof_df = lof_rdd.toDF([\"id\", \"lof\"]).persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "print(\"top-5 outliers :\")\n",
    "lof_df.orderBy(\"lof\", ascending=False).show(5, truncate=False)\n",
    "# lof_df.write.mode(\"overwrite\").option(\"header\", True).csv(\"output/lof_results\")"
   ],
   "id": "34f04fa25a64388a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `lof`.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPySparkTypeError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/BigDataProjEnv/lib/python3.11/site-packages/pyspark/sql/types.py:1644\u001B[39m, in \u001B[36m_infer_type\u001B[39m\u001B[34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[39m\n\u001B[32m   1643\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1644\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _infer_schema(\n\u001B[32m   1645\u001B[39m         obj,\n\u001B[32m   1646\u001B[39m         infer_dict_as_struct=infer_dict_as_struct,\n\u001B[32m   1647\u001B[39m         infer_array_from_first_element=infer_array_from_first_element,\n\u001B[32m   1648\u001B[39m     )\n\u001B[32m   1649\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/BigDataProjEnv/lib/python3.11/site-packages/pyspark/sql/types.py:1684\u001B[39m, in \u001B[36m_infer_schema\u001B[39m\u001B[34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[39m\n\u001B[32m   1683\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1684\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[32m   1685\u001B[39m         error_class=\u001B[33m\"\u001B[39m\u001B[33mCANNOT_INFER_SCHEMA_FOR_TYPE\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1686\u001B[39m         message_parameters={\u001B[33m\"\u001B[39m\u001B[33mdata_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(row).\u001B[34m__name__\u001B[39m},\n\u001B[32m   1687\u001B[39m     )\n\u001B[32m   1689\u001B[39m fields = []\n",
      "\u001B[31mPySparkTypeError\u001B[39m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `float64`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mPySparkTypeError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/BigDataProjEnv/lib/python3.11/site-packages/pyspark/sql/types.py:1695\u001B[39m, in \u001B[36m_infer_schema\u001B[39m\u001B[34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[39m\n\u001B[32m   1691\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1692\u001B[39m     fields.append(\n\u001B[32m   1693\u001B[39m         StructField(\n\u001B[32m   1694\u001B[39m             k,\n\u001B[32m-> \u001B[39m\u001B[32m1695\u001B[39m             _infer_type(\n\u001B[32m   1696\u001B[39m                 v,\n\u001B[32m   1697\u001B[39m                 infer_dict_as_struct,\n\u001B[32m   1698\u001B[39m                 infer_array_from_first_element,\n\u001B[32m   1699\u001B[39m                 prefer_timestamp_ntz,\n\u001B[32m   1700\u001B[39m             ),\n\u001B[32m   1701\u001B[39m             \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m   1702\u001B[39m         )\n\u001B[32m   1703\u001B[39m     )\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/BigDataProjEnv/lib/python3.11/site-packages/pyspark/sql/types.py:1650\u001B[39m, in \u001B[36m_infer_type\u001B[39m\u001B[34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[39m\n\u001B[32m   1649\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1650\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[32m   1651\u001B[39m         error_class=\u001B[33m\"\u001B[39m\u001B[33mUNSUPPORTED_DATA_TYPE\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1652\u001B[39m         message_parameters={\u001B[33m\"\u001B[39m\u001B[33mdata_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(obj).\u001B[34m__name__\u001B[39m},\n\u001B[32m   1653\u001B[39m     )\n",
      "\u001B[31mPySparkTypeError\u001B[39m: [UNSUPPORTED_DATA_TYPE] Unsupported DataType `float64`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mPySparkTypeError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      1\u001B[39m bc_lrd = spark.sparkContext.broadcast(lrd.collectAsMap())\n\u001B[32m      3\u001B[39m lof_rdd = knn.map(\n\u001B[32m      4\u001B[39m     \u001B[38;5;28;01mlambda\u001B[39;00m item: (\n\u001B[32m      5\u001B[39m         item[\u001B[32m0\u001B[39m],\n\u001B[32m      6\u001B[39m         \u001B[38;5;28msum\u001B[39m(bc_lrd.value[j] \u001B[38;5;28;01mfor\u001B[39;00m _, j \u001B[38;5;129;01min\u001B[39;00m item[\u001B[32m1\u001B[39m]) / (k * bc_lrd.value[item[\u001B[32m0\u001B[39m]])\n\u001B[32m      7\u001B[39m     )\n\u001B[32m      8\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m lof_df = lof_rdd.toDF([\u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mlof\u001B[39m\u001B[33m\"\u001B[39m]).persist(StorageLevel.MEMORY_ONLY)\n\u001B[32m     12\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mtop-5 outliers :\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     13\u001B[39m lof_df.orderBy(\u001B[33m\"\u001B[39m\u001B[33mlof\u001B[39m\u001B[33m\"\u001B[39m, ascending=\u001B[38;5;28;01mFalse\u001B[39;00m).show(\u001B[32m5\u001B[39m, truncate=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/BigDataProjEnv/lib/python3.11/site-packages/pyspark/sql/session.py:122\u001B[39m, in \u001B[36m_monkey_patch_RDD.<locals>.toDF\u001B[39m\u001B[34m(self, schema, sampleRatio)\u001B[39m\n\u001B[32m     87\u001B[39m \u001B[38;5;129m@no_type_check\u001B[39m\n\u001B[32m     88\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtoDF\u001B[39m(\u001B[38;5;28mself\u001B[39m, schema=\u001B[38;5;28;01mNone\u001B[39;00m, sampleRatio=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m     89\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     90\u001B[39m \u001B[33;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001B[39;00m\n\u001B[32m     91\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    120\u001B[39m \u001B[33;03m    +---+\u001B[39;00m\n\u001B[32m    121\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m sparkSession.createDataFrame(\u001B[38;5;28mself\u001B[39m, schema, sampleRatio)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/BigDataProjEnv/lib/python3.11/site-packages/pyspark/sql/session.py:1443\u001B[39m, in \u001B[36mSparkSession.createDataFrame\u001B[39m\u001B[34m(self, data, schema, samplingRatio, verifySchema)\u001B[39m\n\u001B[32m   1438\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd.DataFrame):\n\u001B[32m   1439\u001B[39m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[32m   1440\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m).createDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[32m   1441\u001B[39m         data, schema, samplingRatio, verifySchema\n\u001B[32m   1442\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1443\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._create_dataframe(\n\u001B[32m   1444\u001B[39m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m   1445\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/BigDataProjEnv/lib/python3.11/site-packages/pyspark/sql/session.py:1483\u001B[39m, in \u001B[36mSparkSession._create_dataframe\u001B[39m\u001B[34m(self, data, schema, samplingRatio, verifySchema)\u001B[39m\n\u001B[32m   1480\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[32m   1482\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, RDD):\n\u001B[32m-> \u001B[39m\u001B[32m1483\u001B[39m     rdd, struct = \u001B[38;5;28mself\u001B[39m._createFromRDD(data.map(prepare), schema, samplingRatio)\n\u001B[32m   1484\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1485\u001B[39m     rdd, struct = \u001B[38;5;28mself\u001B[39m._createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/BigDataProjEnv/lib/python3.11/site-packages/pyspark/sql/session.py:1056\u001B[39m, in \u001B[36mSparkSession._createFromRDD\u001B[39m\u001B[34m(self, rdd, schema, samplingRatio)\u001B[39m\n\u001B[32m   1052\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1053\u001B[39m \u001B[33;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001B[39;00m\n\u001B[32m   1054\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1055\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[32m-> \u001B[39m\u001B[32m1056\u001B[39m     struct = \u001B[38;5;28mself\u001B[39m._inferSchema(rdd, samplingRatio, names=schema)\n\u001B[32m   1057\u001B[39m     converter = _create_converter(struct)\n\u001B[32m   1058\u001B[39m     tupled_rdd = rdd.map(converter)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/BigDataProjEnv/lib/python3.11/site-packages/pyspark/sql/session.py:1007\u001B[39m, in \u001B[36mSparkSession._inferSchema\u001B[39m\u001B[34m(self, rdd, samplingRatio, names)\u001B[39m\n\u001B[32m   1005\u001B[39m prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n\u001B[32m   1006\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m samplingRatio \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1007\u001B[39m     schema = _infer_schema(\n\u001B[32m   1008\u001B[39m         first,\n\u001B[32m   1009\u001B[39m         names=names,\n\u001B[32m   1010\u001B[39m         infer_dict_as_struct=infer_dict_as_struct,\n\u001B[32m   1011\u001B[39m         prefer_timestamp_ntz=prefer_timestamp_ntz,\n\u001B[32m   1012\u001B[39m     )\n\u001B[32m   1013\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n\u001B[32m   1014\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m rdd.take(\u001B[32m100\u001B[39m)[\u001B[32m1\u001B[39m:]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/BigDataProjEnv/lib/python3.11/site-packages/pyspark/sql/types.py:1705\u001B[39m, in \u001B[36m_infer_schema\u001B[39m\u001B[34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[39m\n\u001B[32m   1692\u001B[39m         fields.append(\n\u001B[32m   1693\u001B[39m             StructField(\n\u001B[32m   1694\u001B[39m                 k,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1702\u001B[39m             )\n\u001B[32m   1703\u001B[39m         )\n\u001B[32m   1704\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1705\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[32m   1706\u001B[39m             error_class=\u001B[33m\"\u001B[39m\u001B[33mCANNOT_INFER_TYPE_FOR_FIELD\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1707\u001B[39m             message_parameters={\u001B[33m\"\u001B[39m\u001B[33mfield_name\u001B[39m\u001B[33m\"\u001B[39m: k},\n\u001B[32m   1708\u001B[39m         )\n\u001B[32m   1709\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m StructType(fields)\n",
      "\u001B[31mPySparkTypeError\u001B[39m: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `lof`."
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Test With Different Number Of Workers",
   "id": "434d36d68db06d35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T12:23:50.701569Z",
     "start_time": "2025-07-05T12:11:42.931619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess, pandas as pd, io, matplotlib.pyplot as plt\n",
    "\n",
    "workers = [1, 2, 4, 8]\n",
    "csv = \"\"\n",
    "for w in workers:\n",
    "    out = subprocess.check_output(\n",
    "        [\"python\", \"benchmark.py\", \"--workers\", str(w)],\n",
    "        text=True\n",
    "    )\n",
    "    csv += out\n",
    "\n",
    "df = pd.read_csv(io.StringIO(csv), header=None, names=[\"workers\", \"time_s\"])\n",
    "df"
   ],
   "id": "edc65bf3fddb7cd8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/05 15:41:46 WARN Utils: Your hostname, Soroush resolves to a loopback address: 127.0.1.1; using 192.168.100.10 instead (on interface wlp5s0)\n",
      "25/07/05 15:41:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/05 15:41:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/05 15:41:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/05 15:42:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/07/05 15:42:24 WARN TaskSetManager: Stage 0 contains a task of very large size (4343 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/05 15:42:26 WARN TaskSetManager: Stage 3 contains a task of very large size (4343 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/05 15:46:50 WARN Utils: Your hostname, Soroush resolves to a loopback address: 127.0.1.1; using 192.168.100.10 instead (on interface wlp5s0)\n",
      "25/07/05 15:46:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/05 15:46:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/05 15:46:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/05 15:47:25 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/07/05 15:47:25 WARN TaskSetManager: Stage 0 contains a task of very large size (2036 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/05 15:47:27 WARN TaskSetManager: Stage 3 contains a task of very large size (2036 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/05 15:49:54 WARN Utils: Your hostname, Soroush resolves to a loopback address: 127.0.1.1; using 192.168.100.10 instead (on interface wlp5s0)\n",
      "25/07/05 15:49:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/05 15:49:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/05 15:49:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/05 15:50:47 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/07/05 15:50:47 WARN TaskSetManager: Stage 0 contains a task of very large size (1355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/05 15:50:49 WARN TaskSetManager: Stage 3 contains a task of very large size (1355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/05 15:52:15 WARN Utils: Your hostname, Soroush resolves to a loopback address: 127.0.1.1; using 192.168.100.10 instead (on interface wlp5s0)\n",
      "25/07/05 15:52:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/05 15:52:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/05 15:52:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/05 15:52:56 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   workers   time_s\n",
       "0        1  301.237\n",
       "1        2  179.620\n",
       "2        4  136.634\n",
       "3        8   92.336"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workers</th>\n",
       "      <th>time_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>301.237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>179.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>136.634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>92.336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T12:32:22.314240Z",
     "start_time": "2025-07-05T12:32:22.311136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.to_csv('results.csv', index=False)\n",
    "df_copy = df"
   ],
   "id": "6952909542ba0758",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4225f22040e49aef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
