{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T10:44:13.099872Z",
     "start_time": "2025-07-06T10:44:10.493482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession, functions as F, Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from operator import add\n",
    "import numpy as np\n",
    "from math import log\n",
    "import logging, os\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import math\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import heapq"
   ],
   "id": "3e764583eef5453e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T10:44:39.172359Z",
     "start_time": "2025-07-06T10:44:32.313104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Dimension Reduction\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ],
   "id": "4933f9a1827ee965",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/06 14:14:36 WARN Utils: Your hostname, Soroush resolves to a loopback address: 127.0.1.1; using 192.168.100.10 instead (on interface wlp5s0)\n",
      "25/07/06 14:14:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/06 14:14:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Import Musk version 2 dataset",
   "id": "c222974093049c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T10:46:18.005594Z",
     "start_time": "2025-07-06T10:46:09.974803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# fetch dataset\n",
    "musk_version_2 = fetch_ucirepo(id=75)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = musk_version_2.data.features\n",
    "y = musk_version_2.data.targets\n",
    "\n",
    "pdf = pd.concat([X, y], axis=1)\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# grab column names\n",
    "label_col = y.columns[0] if hasattr(y, \"columns\") else \"class\"\n",
    "feature_cols = [c for c in df.columns if c != label_col]"
   ],
   "id": "1fec2d6869cd90e5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Scale With Max_Min Normalization method",
   "id": "23748e5082f48ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T10:47:36.489301Z",
     "start_time": "2025-07-06T10:47:32.482901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "assembled_df = assembler.transform(df)\n",
    "\n",
    "# apply spark built-in min-max scaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "df_scaled = scaler_model.transform(assembled_df)\n",
    "\n"
   ],
   "id": "9879cdf7d463a251",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/06 14:17:34 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Density Based Representation",
   "id": "45868de071e2d9c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T10:47:38.377550Z",
     "start_time": "2025-07-06T10:47:38.264781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# number of bins per feature\n",
    "p = 10\n",
    "\n",
    "cube_counts = (\n",
    "    df_scaled.select(\"scaledFeatures\").rdd\n",
    "    # MAP: vector → tuple of bin indices\n",
    "    .map(lambda row: tuple(int(min(x * p, p - 1)) for x in row.scaledFeatures))\n",
    "    # MAP: tuple → (cube_id, 1)\n",
    "    .map(lambda bins: (\"_\".join(map(str, bins)), 1))\n",
    "    # REDUCE: sum counts per cube_id\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n"
   ],
   "id": "68bd1ed668bb3092",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T10:47:43.622281Z",
     "start_time": "2025-07-06T10:47:40.094596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_feats = len(feature_cols)\n",
    "\n",
    "density_df = spark.createDataFrame(\n",
    "    cube_counts.map(lambda kv: Row(cube_id=kv[0], density=kv[1]))\n",
    ")\n",
    "\n",
    "for i in range(num_feats):\n",
    "    density_df = density_df.withColumn(\n",
    "        f\"g{i}\", F.split(F.col(\"cube_id\"), \"_\")[i].cast(\"int\")\n",
    "    )\n"
   ],
   "id": "502212e53784215e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### mRMD-Based Relevant Subspace Selection",
   "id": "f26e159fe869b7eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T10:48:20.498541Z",
     "start_time": "2025-07-06T10:48:17.769200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RDD format\n",
    "col_names = [f\"g{i}\" for i in range(num_feats)]\n",
    "mr_rdd    = density_df.select(col_names + [\"density\"]).rdd.cache()\n",
    "N_total   = mr_rdd.count()\n",
    "\n",
    "# calculate similarity\n",
    "fd_counts = (\n",
    "    mr_rdd.flatMap(\n",
    "        lambda row: [((j, getattr(row, col_names[j]), row.density), 1) for j in range(num_feats)]\n",
    "    ).reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n",
    "feat_marg = fd_counts.map(lambda kv: ((kv[0][0], kv[0][1]), kv[1])).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "dens_marg = fd_counts.map(lambda kv: (kv[0][2], kv[1])).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "fd_list      = fd_counts.collect()\n",
    "feat_dict    = dict(feat_marg.collect())\n",
    "dens_dict    = dict(dens_marg.collect())\n",
    "\n",
    "mi_relevance = {}\n",
    "for (j, gval, dc), cnt in fd_list:\n",
    "    p_joint = cnt / N_total\n",
    "    p_g     = feat_dict[(j, gval)] / N_total\n",
    "    p_d     = dens_dict[dc] / N_total\n",
    "    mi_relevance[j] = mi_relevance.get(j, 0.0) + p_joint * math.log2(p_joint / (p_g * p_d))\n"
   ],
   "id": "7900ba1abfd3cda7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Compute I(gi,gj) And Redundancy",
   "id": "ffbc710b43d0fd91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T10:49:04.462552Z",
     "start_time": "2025-07-06T10:48:25.414437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pair_counts = (\n",
    "    mr_rdd.flatMap(\n",
    "        lambda row: [(((j, l, getattr(row, col_names[j]), getattr(row, col_names[l]))), 1)\n",
    "                      for j in range(num_feats) for l in range(j + 1, num_feats)]\n",
    "    ).reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n",
    "# aggregate pair dictionaries ( (j , l) , ( (v1,v2),cnt))\n",
    "from collections import defaultdict\n",
    "pair_dict = defaultdict(list)\n",
    "for ((j, l, vj, vl), c) in pair_counts.collect():\n",
    "    pair_dict[(j, l)].append(((vj, vl), c))\n",
    "\n",
    "# compute mutual information\n",
    "mi_pair = {}\n",
    "for (j, l), items in pair_dict.items():\n",
    "    score = 0.0\n",
    "    for (vj, vl), cnt in items:\n",
    "        p_joint = cnt / N_total\n",
    "        p_j     = feat_dict[(j, vj)] / N_total\n",
    "        p_l     = feat_dict[(l, vl)] / N_total\n",
    "        score  += p_joint * math.log2(p_joint / (p_j * p_l))\n",
    "    mi_pair[(j, l)] = score\n",
    "    mi_pair[(l, j)] = score"
   ],
   "id": "1b3176127ccff12",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Greedy mRMD Selection",
   "id": "465560a1f3d1f9fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T11:12:29.551875Z",
     "start_time": "2025-07-06T11:12:29.545711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# select the number of desired subspace\n",
    "subspace_size = 10\n",
    "selected, remaining = [], list(range(num_feats))\n",
    "\n",
    "while remaining and len(selected) < subspace_size:\n",
    "    best, best_score = None, float(\"-inf\")\n",
    "    for cand in remaining:\n",
    "        redund = 0.0\n",
    "        if selected:\n",
    "            redund = sum(mi_pair.get((cand, s), 0.0) for s in selected) / len(selected)\n",
    "        score = mi_relevance[cand] - redund\n",
    "        if score > best_score:\n",
    "            best, best_score = cand, score\n",
    "    selected.append(best)\n",
    "    remaining.remove(best)\n",
    "\n",
    "# see the selected features\n",
    "print(\"\\nSelected features (MRMD order):\", [f\"g{i}\" for i in selected])"
   ],
   "id": "82600542849d40d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features (MRMD order): ['g44', 'g4', 'g146', 'g101', 'g156', 'g145', 'g93', 'g144', 'g66', 'g110']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Stage 5: Data Mapping",
   "id": "70fdab05d2af9fb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T11:12:46.307104Z",
     "start_time": "2025-07-06T11:12:43.473959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Convert 'gXX' to integer indices and broadcast for tiny closures\n",
    "indexes = [\n",
    "    col if isinstance(col, int) else int(col[1:])   # 'g12' → 12, 12 → 12\n",
    "    for col in selected\n",
    "]\n",
    "bc_idx = spark.sparkContext.broadcast(indexes)\n",
    "\n",
    "# Add a row-id so we can join later if needed\n",
    "norm_rdd = (\n",
    "    df_scaled.rdd\n",
    "            .zipWithIndex()\n",
    "            .map(lambda t: (t[1], t[0].scaledFeatures))\n",
    ")\n",
    "\n",
    "proj_rdd = (\n",
    "    norm_rdd\n",
    "      .map(lambda kv: (kv[0],\n",
    "                       [kv[1][i] for i in bc_idx.value]))\n",
    ")\n",
    "\n",
    "\n",
    "subspace_df = (\n",
    "    proj_rdd\n",
    "      .map(lambda kv: (kv[0], Vectors.dense(kv[1])))\n",
    "      .toDF([\"id\", \"subspaceFeatures\"])\n",
    ")\n",
    "\n",
    "subspace_df.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "print(f\"✓ Data-Mapping complete – projected into {len(indexes)}-D sub-space.\")\n",
    "subspace_df.show(5, truncate=False)\n"
   ],
   "id": "88d1cc827c79c322",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data-Mapping complete – projected into 10-D sub-space.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |subspaceFeatures                                                                                                                                                                                     |\n",
      "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |[0.061269146608315096,0.002257336343115124,0.06493506493506494,0.47506561679790027,0.03233830845771144,0.013623978201634877,0.03125,0.00510204081632653,0.0024937655860349127,0.02396514161220044]   |\n",
      "|1  |[0.15098468271334792,0.002257336343115124,0.048701298701298704,0.4461942257217848,0.05472636815920398,0.010899182561307902,0.443359375,0.00510204081632653,0.0024937655860349127,0.04139433551198257]|\n",
      "|2  |[0.1487964989059081,0.002257336343115124,0.0551948051948052,0.4068241469816273,0.0472636815920398,0.010899182561307902,0.474609375,0.00510204081632653,0.0024937655860349127,0.04357298474945534]    |\n",
      "|3  |[0.1487964989059081,0.002257336343115124,0.048701298701298704,0.4461942257217848,0.05223880597014925,0.013623978201634877,0.443359375,0.00510204081632653,0.0024937655860349127,0.04139433551198257] |\n",
      "|4  |[0.1487964989059081,0.002257336343115124,0.048701298701298704,0.4461942257217848,0.05223880597014925,0.013623978201634877,0.443359375,0.00510204081632653,0.0024937655860349127,0.04139433551198257] |\n",
      "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Stage 6: Compute LOF Scores\n",
    "#### First Find K Nearest Neighbors"
   ],
   "id": "143aa64fc4244767"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T11:13:13.418139Z",
     "start_time": "2025-07-06T11:13:13.389249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k = 50\n",
    "rdd = subspace_df.rdd.map(lambda r: (r.id, r.subspaceFeatures))\n",
    "\n",
    "pairs = (\n",
    "    rdd.cartesian(rdd)\n",
    "        .filter(lambda t: t[0][0] != t[1][0])\n",
    "        .map(lambda t: ( t[0][0],\n",
    "                         ( Vectors.squared_distance(t[0][1], t[1][1]),\n",
    "                           t[1][0]) ))\n",
    ")\n",
    "\n",
    "def top_k(acc, x):\n",
    "    if len(acc) < k:\n",
    "        heapq.heappush(acc, (-x[0], x[1]))           # max-heap keeps k smallest\n",
    "    else:\n",
    "        heapq.heappushpop(acc, (-x[0], x[1]))\n",
    "    return acc\n",
    "\n",
    "knn = pairs.aggregateByKey([], top_k,\n",
    "                           lambda a, b: heapq.nsmallest(k, a + b, key=lambda t: -t[0]))\n"
   ],
   "id": "ab675b9e33094fc0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Compute Local Reachability Density",
   "id": "85aa66d63467ed52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T11:14:07.345008Z",
     "start_time": "2025-07-06T11:13:16.674030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kdist = knn.mapValues(lambda lst: max(-d2 for d2, _ in lst)).collectAsMap()\n",
    "bc_kd  = spark.sparkContext.broadcast(kdist)\n",
    "\n",
    "reach_rdd = knn.flatMap(\n",
    "    lambda item: [\n",
    "        (item[0], (max(-d2, bc_kd.value[j]), 1.0))\n",
    "        for d2, j in item[1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "lrd = (reach_rdd\n",
    "       .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "       .mapValues(lambda s: s[1] / s[0]))\n"
   ],
   "id": "eec61e60c185b6ff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### LOF(i) = Σ LRD(j) / (k * LRD(i)",
   "id": "ff1dc8514fc377fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T00:53:55.088012Z",
     "start_time": "2025-07-06T00:53:50.034408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "bc_lrd = spark.sparkContext.broadcast(lrd.collectAsMap())\n",
    "k      = 50\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\",  LongType(),   False),\n",
    "    StructField(\"lof\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "lof_rdd = knn.map(\n",
    "    lambda item: (\n",
    "        int(item[0]),\n",
    "        float(\n",
    "            sum(bc_lrd.value[j] for _, j in item[1]) /\n",
    "            (k * bc_lrd.value[item[0]])\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "lof_df = (spark.createDataFrame(lof_rdd, schema)\n",
    "                 .persist(StorageLevel.MEMORY_ONLY))\n",
    "\n",
    "print(\"top-5 outliers (highest LOF):\")\n",
    "lof_df.orderBy(\"lof\", ascending=False).show(5, truncate=False)\n",
    "# lof_df.write.mode(\"overwrite\").option(\"header\", True).csv(\"output/lof_results\")\n"
   ],
   "id": "34f04fa25a64388a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-5 outliers (highest LOF):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===============================================>     (231 + 16) / 256]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|id |lof               |\n",
      "+---+------------------+\n",
      "|807|18.935226716433124|\n",
      "|243|17.948039433107876|\n",
      "|241|17.946894738778937|\n",
      "|242|17.946894738778937|\n",
      "|796|17.90126260611107 |\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T00:54:58.895660Z",
     "start_time": "2025-07-06T00:54:57.373552Z"
    }
   },
   "cell_type": "code",
   "source": "lof_df.write.mode(\"overwrite\").option(\"header\", True).csv(\"output/lof_results\")",
   "id": "2156474d9b763adb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Test With Different Number Of Workers",
   "id": "434d36d68db06d35"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-07T00:40:41.678889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "pdf = pd.read_csv(\"musk2.csv\")\n",
    "csv_path = 'musk2.csv'\n",
    "\n",
    "import subprocess, re\n",
    "\n",
    "workers_list = [1, 2, 4, 8, 16,20,26,32,64]\n",
    "results = []\n",
    "\n",
    "for w in workers_list:\n",
    "    print(f\"→ Running with {w} worker{'s' if w>1 else ''}…\")\n",
    "    out = subprocess.check_output(\n",
    "        f\"python pipeline.py --csv {csv_path} --workers {w}\",\n",
    "        shell=True, stderr=subprocess.STDOUT\n",
    "    ).decode()\n",
    "    print(out)\n",
    "    # parse Time=…s and AUC=…\n",
    "    t_sec = float(re.search(r\"Time=(\\d+\\.\\d+)s\", out).group(1))\n",
    "    auc   = float(re.search(r\"AUC=(\\d+\\.\\d+)\", out).group(1))\n",
    "    results.append({\"workers\": w, \"time_s\": t_sec, \"auc\": auc})\n",
    "\n",
    "#Build a DataFrame of results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ],
   "id": "edc65bf3fddb7cd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Running with 1 worker…\n",
      "25/07/07 04:10:54 WARN Utils: Your hostname, Soroush resolves to a loopback address: 127.0.1.1; using 192.168.100.10 instead (on interface wlp5s0)\n",
      "25/07/07 04:10:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/07 04:10:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/07 04:10:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/07/07 04:10:59 WARN TaskSetManager: Stage 0 contains a task of very large size (4343 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/07 04:11:00 WARN TaskSetManager: Stage 3 contains a task of very large size (4343 KiB). The maximum recommended task size is 1000 KiB.\n",
      "Selected features (mRMRD order): [44, 4, 146, 101, 156, 145, 93, 144, 66, 110]  \n",
      "25/07/07 04:15:14 WARN TaskSetManager: Stage 21 contains a task of very large size (4343 KiB). The maximum recommended task size is 1000 KiB.\n",
      "✓ Data mapping complete – projected into 10-D subspace.\n",
      "25/07/07 04:15:15 WARN TaskSetManager: Stage 22 contains a task of very large size (4343 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/07 04:15:16 WARN TaskSetManager: Stage 23 contains a task of very large size (4343 KiB). The maximum recommended task size is 1000 KiB.\n",
      "+---+------------------+                                                        \n",
      "|id |lof               |\n",
      "+---+------------------+\n",
      "|807|18.935226716433117|\n",
      "|243|17.948039433107887|\n",
      "|241|17.946894738778937|\n",
      "|242|17.946894738778937|\n",
      "|796|17.901262606111064|\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      ">> Workers=1, Time=368.94s, AUC=0.4891\n",
      "\n",
      "→ Running with 2 workers…\n",
      "25/07/07 04:17:04 WARN Utils: Your hostname, Soroush resolves to a loopback address: 127.0.1.1; using 192.168.100.10 instead (on interface wlp5s0)\n",
      "25/07/07 04:17:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/07 04:17:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/07 04:17:09 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/07/07 04:17:09 WARN TaskSetManager: Stage 0 contains a task of very large size (2036 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/07 04:17:10 WARN TaskSetManager: Stage 3 contains a task of very large size (2036 KiB). The maximum recommended task size is 1000 KiB.\n",
      "Selected features (mRMRD order): [44, 4, 146, 101, 156, 145, 93, 144, 66, 110]  \n",
      "25/07/07 04:19:35 WARN TaskSetManager: Stage 21 contains a task of very large size (2036 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/07 04:19:36 WARN TaskSetManager: Stage 22 contains a task of very large size (2036 KiB). The maximum recommended task size is 1000 KiB.\n",
      "✓ Data mapping complete – projected into 10-D subspace.\n",
      "25/07/07 04:19:36 WARN TaskSetManager: Stage 23 contains a task of very large size (2036 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/07 04:19:37 WARN TaskSetManager: Stage 24 contains a task of very large size (2036 KiB). The maximum recommended task size is 1000 KiB.\n",
      "+---+------------------+                                                        \n",
      "|id |lof               |\n",
      "+---+------------------+\n",
      "|807|18.935226716433124|\n",
      "|243|17.948039433107876|\n",
      "|242|17.946894738778937|\n",
      "|241|17.946894738778937|\n",
      "|796|17.90126260611107 |\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      ">> Workers=2, Time=217.72s, AUC=0.4891\n",
      "\n",
      "→ Running with 4 workers…\n",
      "25/07/07 04:20:43 WARN Utils: Your hostname, Soroush resolves to a loopback address: 127.0.1.1; using 192.168.100.10 instead (on interface wlp5s0)\n",
      "25/07/07 04:20:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/07 04:20:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/07 04:20:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/07/07 04:20:48 WARN TaskSetManager: Stage 0 contains a task of very large size (1355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/07 04:20:50 WARN TaskSetManager: Stage 3 contains a task of very large size (1355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "Selected features (mRMRD order): [44, 4, 146, 101, 156, 145, 93, 144, 66, 110]  \n",
      "25/07/07 04:22:08 WARN TaskSetManager: Stage 21 contains a task of very large size (1355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "✓ Data mapping complete – projected into 10-D subspace.                         \n",
      "25/07/07 04:22:09 WARN TaskSetManager: Stage 23 contains a task of very large size (1355 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/07 04:22:10 WARN TaskSetManager: Stage 24 contains a task of very large size (2036 KiB). The maximum recommended task size is 1000 KiB.\n",
      "+---+------------------+                                                        \n",
      "|id |lof               |\n",
      "+---+------------------+\n",
      "|807|18.935226716433124|\n",
      "|243|17.948039433107876|\n",
      "|241|17.946894738778937|\n",
      "|242|17.946894738778937|\n",
      "|796|17.90126260611107 |\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      ">> Workers=4, Time=124.63s, AUC=0.4891\n",
      "\n",
      "→ Running with 8 workers…\n",
      "25/07/07 04:22:48 WARN Utils: Your hostname, Soroush resolves to a loopback address: 127.0.1.1; using 192.168.100.10 instead (on interface wlp5s0)\n",
      "25/07/07 04:22:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/07 04:22:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/07 04:22:53 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "Selected features (mRMRD order): [44, 4, 146, 101, 156, 145, 93, 144, 66, 110]  \n",
      "✓ Data mapping complete – projected into 10-D subspace.                         \n",
      "25/07/07 04:23:44 WARN TaskSetManager: Stage 24 contains a task of very large size (1101 KiB). The maximum recommended task size is 1000 KiB.\n",
      "+---+------------------+                                                        \n",
      "|id |lof               |\n",
      "+---+------------------+\n",
      "|807|18.935226716433124|\n",
      "|243|17.948039433107876|\n",
      "|242|17.946894738778937|\n",
      "|241|17.946894738778937|\n",
      "|796|17.90126260611107 |\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      ">> Workers=8, Time=138.79s, AUC=0.4891\n",
      "\n",
      "→ Running with 16 workers…\n",
      "25/07/07 04:25:09 WARN Utils: Your hostname, Soroush resolves to a loopback address: 127.0.1.1; using 192.168.100.10 instead (on interface wlp5s0)\n",
      "25/07/07 04:25:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/07 04:25:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/07 04:25:14 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "Selected features (mRMRD order): [44, 4, 146, 101, 156, 145, 93, 144, 66, 110]  \n",
      "✓ Data mapping complete – projected into 10-D subspace.                         \n",
      "+---+------------------+                                                        \n",
      "|id |lof               |\n",
      "+---+------------------+\n",
      "|807|18.935226716433124|\n",
      "|243|17.948039433107876|\n",
      "|241|17.946894738778937|\n",
      "|242|17.946894738778937|\n",
      "|796|17.90126260611107 |\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      ">> Workers=16, Time=129.23s, AUC=0.4891\n",
      "\n",
      "→ Running with 20 workers…\n",
      "25/07/07 04:27:19 WARN Utils: Your hostname, Soroush resolves to a loopback address: 127.0.1.1; using 192.168.100.10 instead (on interface wlp5s0)\n",
      "25/07/07 04:27:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/07 04:27:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/07 04:27:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "Selected features (mRMRD order): [44, 4, 146, 101, 156, 145, 93, 144, 66, 110]  \n",
      "✓ Data mapping complete – projected into 10-D subspace.                         \n",
      "+---+------------------+                                                        \n",
      "|id |lof               |\n",
      "+---+------------------+\n",
      "|807|18.935226716433124|\n",
      "|243|17.948039433107876|\n",
      "|242|17.946894738778937|\n",
      "|241|17.946894738778937|\n",
      "|796|17.90126260611107 |\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      ">> Workers=20, Time=147.37s, AUC=0.4891\n",
      "\n",
      "→ Running with 26 workers…\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T12:32:22.314240Z",
     "start_time": "2025-07-05T12:32:22.311136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.to_csv('results.csv', index=False)\n",
    "df_copy = df"
   ],
   "id": "6952909542ba0758",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T01:39:38.368390Z",
     "start_time": "2025-07-06T01:39:38.365529Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Spark version:\", spark.version)",
   "id": "4225f22040e49aef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.4\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6c2f3b79f01b753e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
