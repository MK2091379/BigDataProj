{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:30:16.625322Z",
     "start_time": "2025-07-05T09:30:16.621887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession, functions as F, Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from operator import add\n",
    "import numpy as np\n",
    "from math import log\n",
    "import logging, os\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import math"
   ],
   "id": "3e764583eef5453e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:30:27.657281Z",
     "start_time": "2025-07-05T09:30:26.480007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Dimension Reduction\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ],
   "id": "4933f9a1827ee965",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Import Musk version 2 dataset",
   "id": "c222974093049c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:31:43.993202Z",
     "start_time": "2025-07-05T09:30:53.040096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# fetch dataset\n",
    "musk_version_2 = fetch_ucirepo(id=75)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = musk_version_2.data.features\n",
    "y = musk_version_2.data.targets\n",
    "\n",
    "pdf = pd.concat([X, y], axis=1)\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# grab column names\n",
    "label_col = y.columns[0] if hasattr(y, \"columns\") else \"class\"\n",
    "feature_cols = [c for c in df.columns if c != label_col]"
   ],
   "id": "1fec2d6869cd90e5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Scale With Max_Min Normalization method",
   "id": "23748e5082f48ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:32:00.499893Z",
     "start_time": "2025-07-05T09:31:55.559189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "assembled_df = assembler.transform(df)\n",
    "\n",
    "# apply spark built-in min-max scaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "df_scaled = scaler_model.transform(assembled_df)\n",
    "\n"
   ],
   "id": "9879cdf7d463a251",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/05 13:01:57 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Density Based Representation",
   "id": "45868de071e2d9c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T10:30:13.078323Z",
     "start_time": "2025-07-05T10:30:12.881597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# number of bins per feature\n",
    "p = 10\n",
    "\n",
    "cube_counts = (\n",
    "    df_scaled.select(\"scaledFeatures\").rdd\n",
    "    # MAP: vector → tuple of bin indices\n",
    "    .map(lambda row: tuple(int(min(x * p, p - 1)) for x in row.scaledFeatures))\n",
    "    # MAP: tuple → (cube_id, 1)\n",
    "    .map(lambda bins: (\"_\".join(map(str, bins)), 1))\n",
    "    # REDUCE: sum counts per cube_id\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n"
   ],
   "id": "68bd1ed668bb3092",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T10:30:17.694243Z",
     "start_time": "2025-07-05T10:30:13.803715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_feats = len(feature_cols)\n",
    "\n",
    "density_df = spark.createDataFrame(\n",
    "    cube_counts.map(lambda kv: Row(cube_id=kv[0], density=kv[1]))\n",
    ")\n",
    "\n",
    "for i in range(num_feats):\n",
    "    density_df = density_df.withColumn(\n",
    "        f\"g{i}\", F.split(F.col(\"cube_id\"), \"_\")[i].cast(\"int\")\n",
    "    )\n"
   ],
   "id": "502212e53784215e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## mRMD-Based Relevant Subspace Selection",
   "id": "f26e159fe869b7eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T10:30:21.515400Z",
     "start_time": "2025-07-05T10:30:18.591980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RDD format\n",
    "col_names = [f\"g{i}\" for i in range(num_feats)]\n",
    "mr_rdd    = density_df.select(col_names + [\"density\"]).rdd.cache()\n",
    "N_total   = mr_rdd.count()\n",
    "\n",
    "# calculate similarity\n",
    "fd_counts = (\n",
    "    mr_rdd.flatMap(\n",
    "        lambda row: [((j, getattr(row, col_names[j]), row.density), 1) for j in range(num_feats)]\n",
    "    ).reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n",
    "feat_marg = fd_counts.map(lambda kv: ((kv[0][0], kv[0][1]), kv[1])).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "dens_marg = fd_counts.map(lambda kv: (kv[0][2], kv[1])).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "fd_list      = fd_counts.collect()\n",
    "feat_dict    = dict(feat_marg.collect())\n",
    "dens_dict    = dict(dens_marg.collect())\n",
    "\n",
    "mi_relevance = {}\n",
    "for (j, gval, dc), cnt in fd_list:\n",
    "    p_joint = cnt / N_total\n",
    "    p_g     = feat_dict[(j, gval)] / N_total\n",
    "    p_d     = dens_dict[dc] / N_total\n",
    "    mi_relevance[j] = mi_relevance.get(j, 0.0) + p_joint * math.log2(p_joint / (p_g * p_d))\n"
   ],
   "id": "7900ba1abfd3cda7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Compute I(gi,gj) And Redundancy",
   "id": "ffbc710b43d0fd91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T10:54:07.581686Z",
     "start_time": "2025-07-05T10:53:29.035154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pair_counts = (\n",
    "    mr_rdd.flatMap(\n",
    "        lambda row: [(((j, l, getattr(row, col_names[j]), getattr(row, col_names[l]))), 1)\n",
    "                      for j in range(num_feats) for l in range(j + 1, num_feats)]\n",
    "    ).reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n",
    "# aggregate pair dictionaries ( (j , l) , ( (v1,v2),cnt))\n",
    "from collections import defaultdict\n",
    "pair_dict = defaultdict(list)\n",
    "for ((j, l, vj, vl), c) in pair_counts.collect():\n",
    "    pair_dict[(j, l)].append(((vj, vl), c))\n",
    "\n",
    "# compute mutual information\n",
    "mi_pair = {}\n",
    "for (j, l), items in pair_dict.items():\n",
    "    score = 0.0\n",
    "    for (vj, vl), cnt in items:\n",
    "        p_joint = cnt / N_total\n",
    "        p_j     = feat_dict[(j, vj)] / N_total\n",
    "        p_l     = feat_dict[(l, vl)] / N_total\n",
    "        score  += p_joint * math.log2(p_joint / (p_j * p_l))\n",
    "    mi_pair[(j, l)] = score\n",
    "    mi_pair[(l, j)] = score"
   ],
   "id": "1b3176127ccff12",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Greedy mRMD Selection",
   "id": "465560a1f3d1f9fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T11:31:57.043028Z",
     "start_time": "2025-07-05T11:31:57.038010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# select the number of desired subspace\n",
    "subspace_size = 10\n",
    "selected, remaining = [], list(range(num_feats))\n",
    "\n",
    "while remaining and len(selected) < subspace_size:\n",
    "    best, best_score = None, float(\"-inf\")\n",
    "    for cand in remaining:\n",
    "        redund = 0.0\n",
    "        if selected:\n",
    "            redund = sum(mi_pair.get((cand, s), 0.0) for s in selected) / len(selected)\n",
    "        score = mi_relevance[cand] - redund\n",
    "        if score > best_score:\n",
    "            best, best_score = cand, score\n",
    "    selected.append(best)\n",
    "    remaining.remove(best)\n",
    "\n",
    "# see the selected features\n",
    "print(\"\\nSelected features (MRMD order):\", [f\"g{i}\" for i in selected])"
   ],
   "id": "82600542849d40d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features (MRMD order): ['g44', 'g4', 'g146', 'g101', 'g156', 'g145', 'g93', 'g144', 'g66', 'g110']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d67aa7770c303c2b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
